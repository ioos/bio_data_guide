{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning Data to Darwin Core - Sampling Event with Measurement or Fact using Python\n",
    "Matt Biddle\n",
    "\n",
    "Adapted by Dylan Pugh\n",
    "\n",
    "November 9, 2020\n",
    "\n",
    "# General information about this notebook\n",
    "This notebook was created for the IOOS DMAC Code Sprint Biological Data Session\n",
    "The data in this notebook were created specifically as an example and meant solely to be\n",
    "illustrative of the process for aligning data to the biological data standard - Darwin Core.\n",
    "These data should not be considered actually occurrences of species and any measurements\n",
    "are also contrived. This notebook is meant to provide a step by step process for taking\n",
    "original data and aligning it to Darwin Core\n",
    "\n",
    "This notebook is a python implementation of the R notebook [IOOS_DMAC_DataToDWC_Notebook_event.R](https://github.com/ioos/bio_data_guide/blob/master/Standardizing%20Marine%20Biological%20Data/datasets/example_script_with_fake_data/IOOS_DMAC_DataToDwC_Notebook_event.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyworms # pip install git+git://github.com/iobis/pyworms.git\n",
    "import numpy as np\n",
    "import uuid\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the raw data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.neracoos.org/erddap/tabledap/WBTS_CFIN_2004_2017.csv\"\n",
    "df = pd.read_csv(url, header=[0])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input columns:\n",
    "1. Cruise_Identification_Tag\n",
    "2. CRUISE_ID\n",
    "3. Station_ID\n",
    "4. latitude\n",
    "5. longitude\n",
    "6. time\n",
    "7. Cast\n",
    "8. Net_Type\n",
    "9. Mesh_Size\n",
    "10. NET_DEPTH\n",
    "11. STATION_DEPTH\n",
    "12. COMMENT\n",
    "13. Plankton_Net_Area\n",
    "14. Volume_Filtered\n",
    "15. Sample_Split\n",
    "16. Sample_Dry_Weight\n",
    "17. DW_G_M_2\n",
    "18. Dilution_Factor\n",
    "19. TOTAL_DILFACTOR_CFIN\n",
    "20. Order\n",
    "21. Calanus_finmarchicus_N\n",
    "22. Calanus_finmarchicus_CI\n",
    "23. Calanus_finmarchicus_CII\n",
    "24. Calanus_finmarchicus_CIII\n",
    "25. Calanus_finmarchicus_CIV\n",
    "26. Calanus_finmarchicus_CV\n",
    "27. Calanus_finmarchicus_F\n",
    "28. Calanus_finmarchicus_M\n",
    "\n",
    "## Mappings:\n",
    "**Event Table**  \n",
    "\n",
    "| *Origin Term*             | *DwC_term(s)*                              |  *Notes*               |\n",
    "|---------------------------|--------------------------------------------|------------------------|\n",
    "| Cruise_Identification_Tag | eventID                                    | eventID                |\n",
    "| CRUISE_ID                 | eventID                                    | contained in eventID   |\n",
    "| Station_ID                | eventID                                    | contained in eventID   |\n",
    "| cast                      | eventID                                    | contained in eventID   |\n",
    "| latitude                  | decimalLatitude                            |                        |\n",
    "| longitude                 | decimalLongitude                           |                        |\n",
    "| STATION_DEPTH             | minimumDepthInMeters, maximumDepthInMeters |                        |\n",
    "| time                      | eventDate                                  |                        |\n",
    "|                           | geodeticDatum                              | added programatically  |\n",
    "|                           | samplingProtocol                           | added manually         |\n",
    "\n",
    "**Occurrence Table**\n",
    "\n",
    "The `Calanus_finmarchius_*` readings are split into individual records, with the following fields added:\n",
    "\n",
    "| *Origin Term*         | *DwC_term(s)*            | *Notes*                           |\n",
    "|-----------------------|--------------------------|-----------------------------------|\n",
    "| `Calanus_finmarchius_*` | individualCount        | original value under each column  |\n",
    "|                       | scientificName           | derived from original column name |\n",
    "|                       | occurrenceStatus         | added programatically             |\n",
    "|                       | lifeStage                | derived from original column name |\n",
    "|                       | sex                      | derived from original column name |\n",
    "|                       | acceptedname             | programatic pyworms lookup        |\n",
    "|                       | acceptedID               | programatic pyworms lookup        |\n",
    "|                       | scientificNameID         | programatic pyworms lookup        |\n",
    "|                       | kingdom                  | programatic pyworms lookup        |\n",
    "|                       | phylum                   | programatic pyworms lookup        |\n",
    "|                       | class                    | programatic pyworms lookup        |\n",
    "|                       | order                    | programatic pyworms lookup        |\n",
    "|                       | family                   | programatic pyworms lookup        |\n",
    "|                       | genus                    | programatic pyworms lookup        |\n",
    "|                       | scientificNameAuthorship | programatic pyworms lookup        |\n",
    "|                       | taxonRank                | programatic pyworms lookup        |\n",
    "|                       | basisOfRecord            | added programatically             |\n",
    "\n",
    "**Measurement or Fact Table**\n",
    "\n",
    "Each entry in this table has the following fields:\n",
    "\n",
    "1. measurementType\n",
    "2. measurementTypeID\n",
    "3. measurementValue\n",
    "4. measurementUnit\n",
    "5. measurementUnitID\n",
    "6. measurementAccuracy\n",
    "7. measurementDeterminedDate\n",
    "8. measurementMethod\n",
    "9. measurementRemark\n",
    "\n",
    "This table shows the mapping from the origin term to the BODC NERC vocabulary term:\n",
    "\n",
    "| *Origin Term*        | *BODC NERC vocabulary/measurementTypeID*                              | *URI*                                                                |\n",
    "|----------------------|-----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| Net_Type             | plankton net                                                          | [22](http://vocab.nerc.ac.uk/collection/L05/current/22/)             |\n",
    "| Mesh_Size            | Sampling net mesh size                                                | [Q0100015](http://vocab.nerc.ac.uk/collection/Q01/current/Q0100015/) |\n",
    "| NET_DEPTH            | Depth (spatial coordinate) of sampling event start                    | [DXPHPRST](http://vocab.nerc.ac.uk/collection/P01/current/DXPHPRST/) |\n",
    "| COMMENT              | N/A (mapped to measurementRemark field above)                         | N/A                                                                  |\n",
    "| Plankton_Net_Area    | Sampling device aperture surface area                                 | [Q0100017](http://vocab.nerc.ac.uk/collection/Q01/current/Q0100017/) |\n",
    "| Volume_Filtered      | Volume                                                                | [VOL](http://vocab.nerc.ac.uk/collection/P25/current/VOL/)           |\n",
    "| Sample_Split         | N/A (information added to measurementRemark field above)              | N/A                                                                  |\n",
    "| Sample_Dry_Weight    | Dry weight biomass                                                    | [ODRYBM01](http://vocab.nerc.ac.uk/collection/P01/current/ODRYBM01/) |\n",
    "| DW_G_M_2             | Dry weight biomass                                                    | [ODRYBM01](http://vocab.nerc.ac.uk/collection/P01/current/ODRYBM01)  |\n",
    "| Dilution_Factor      | ???                                                                   | ???                                                                  |\n",
    "| TOTAL_DILFACTOR_CFIN | ???                                                                   | ???                                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to to decide if we will provide an occurrence only version of the data or\n",
    "a sampling event with measurement or facts version of the data. Occurrence only is easier\n",
    "to create. It's only one file to produce. However, several pieces of information will be\n",
    "left out if we choose that option. If we choose to do sampling event with measurement or\n",
    "fact we'll be able to capture all of the data in the file creating a lossless version.\n",
    "Here we decide to use the sampling event option to include as much information as we can.\n",
    "\n",
    "First let's create the eventID and occurrenceID in the original file so that information\n",
    "can be reused for all necessary files down the line. Luckily, our data already has an appropriate eventID in the `Cruise_Identification_Tag` field, so we'll use that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eventID'] = df['Cruise_Identification_Tag']\n",
    "df['occurrenceID'] = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create three separate files to comply with the sampling event format.\n",
    "We'll start with the event file but we only need to include the columns that are relevant\n",
    "to the event file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = df[['time', 'latitude', 'longitude', 'NET_DEPTH', 'STATION_DEPTH', 'eventID']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to rename any columns of data that match directly to Darwin Core. We know\n",
    "this based on our crosswalk spreadsheet CrosswalkToDarwinCore.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event['eventDate'] = event['time']\n",
    "event['decimalLatitude'] = event['latitude']\n",
    "event['decimalLongitude'] = event['longitude']\n",
    "event['minimumDepthInMeters'] = event['NET_DEPTH']\n",
    "event['maximumDepthInMeters'] = event['NET_DEPTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have to add any missing required fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a guess\n",
    "event['geodeticDatum'] = 'EPSG:4326 WGS84'\n",
    "# this is found in the metadata\n",
    "event['samplingProtocol'] = 'Mesh net cast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll remove any columns that we no longer need to clean things up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.drop(\n",
    "    columns=['latitude', 'longitude', 'NET_DEPTH', 'time'],\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have too many repeating rows of information. We can pare this down using eventID which\n",
    "is a unique identifier for each sampling event in the data- which is six, three transects\n",
    "per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.drop_duplicates(\n",
    "    subset='eventID',\n",
    "    inplace=True)\n",
    "\n",
    "event.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we write out the event file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.to_csv(\n",
    "    'data/processed/WBTS_CFIN_2004_2017_event_frompy.csv',\n",
    "    header=True,\n",
    "    index=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occurrence file\n",
    "Next we need to create the occurrence file. We start by examining the structure (columns) of the source data. The goal here is to assess what kind of conversion (if any) will be necessary for Darwin Core alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `Calanus_finmarchicus` columns need to be converted into a more suitable format. We need to iterate through the existing data row by row - the goal is to create five new columns: `scientificName`, `lifeStage`, `sex`, `occuranceStatus`, & `individualCount`.\n",
    "\n",
    "We start by isolating the records that have valid data. We define the columns we want to check against as `target_data_columns`, and then create a new dataframe `calanus_records` by retaining only records where at least one of the columns has a value of NOT `0` AND NOT `NaN`.\n",
    "\n",
    "We also drop the second row, which contains unit information to avoid confusing the parser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data_columns = ['Calanus_finmarchicus_N',\n",
    "                       'Calanus_finmarchicus_CI',\n",
    "                       'Calanus_finmarchicus_CII',\n",
    "                       'Calanus_finmarchicus_CIII',\n",
    "                       'Calanus_finmarchicus_CIV',\n",
    "                       'Calanus_finmarchicus_CV',\n",
    "                       'Calanus_finmarchicus_F',\n",
    "                       'Calanus_finmarchicus_M']\n",
    "\n",
    "# drop units row from calanus records\n",
    "\n",
    "calanus_records = df.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is that, in its current form, each row actually represents between 0 and 8 discrete occurances. This isn't suitable for Darwin Core, so we need to read each row, and then split its data into new records, each representing an occurance event. This is a little tricky, so we'll create a helper method `enumerate_row` which takes a row (a `pandas.Series` object in practice) and makes the appropriate transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_row(row, field):\n",
    "    # expands rows which contain multiple observations into discrete records\n",
    "    row_data = row[1]\n",
    "    calanus_count = row_data[field]\n",
    "\n",
    "    # convert to dict so we can mutate\n",
    "    enumerated_row = row_data.to_dict()\n",
    "\n",
    "    split_species = field.rsplit('_', 1)\n",
    "    scientific_name = split_species[0].replace('_', ' ')\n",
    "    life_stage = split_species[1]\n",
    "\n",
    "    # add count of specified species as a new column\n",
    "    enumerated_row['individualCount'] = calanus_count\n",
    "    enumerated_row['scientificName'] = scientific_name\n",
    "    \n",
    "    enumerated_row['occurrenceStatus'] = 'present' if pd.to_numeric(calanus_count) > 0 and calanus_count != 'NaN' else 'absent'\n",
    "\n",
    "    life_stage = field.rsplit('_', 1)[1]\n",
    "    if life_stage == 'N':\n",
    "        life_stage = 'Nauplius'\n",
    "    enumerated_row['lifeStage'] = life_stage if life_stage != 'F' and life_stage != 'M' else 'adult'\n",
    "\n",
    "    # this is consistent across records\n",
    "    enumerated_row['basisOfRecord'] = 'HumanObservation'\n",
    "\n",
    "    if life_stage == 'F':\n",
    "        enumerated_row['sex'] = 'female'\n",
    "    elif life_stage == 'M':\n",
    "        enumerated_row['sex'] = 'male'\n",
    "    else:\n",
    "        enumerated_row['sex'] = 'NA'\n",
    "\n",
    "    return enumerated_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to loop through the target data. The top-level control variable is the list of the columns we wish to enumerate, so we will look for each target column in each row of the dataset. \n",
    "\n",
    "*note*: This operation could easily become costly depending on the number of rows and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerated_rows = []\n",
    "# loop through target column list\n",
    "for field in target_data_columns:\n",
    "\n",
    "    # now enumerate each input row, extracting the values\n",
    "    for row in calanus_records.iterrows():\n",
    "\n",
    "        flipped_row = enumerate_row(row, field)\n",
    "\n",
    "        # delete other calanus records from flipped row\n",
    "        for k in target_data_columns:\n",
    "            flipped_row.pop(k, None)\n",
    "\n",
    "        enumerated_rows.append(flipped_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert the list of dicts into a dataframe\n",
    "output_frame = pd.DataFrame.from_dict(enumerated_rows)\n",
    "\n",
    "# sort by time, ascending\n",
    "output_frame.sort_values(by='time', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data should be in a more suitable fromat, so we can proceed. \n",
    "\n",
    "We start by creating a new occurrence data frame with the relevant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence = output_frame[['scientificName', 'eventID', 'occurrenceID', 'individualCount', 'occurrenceStatus', 'lifeStage', 'sex']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic Name Matching\n",
    "A requirement for OBIS is that all scientific names match to the World Register of\n",
    "Marine Species (WoRMS) and a scientificNameID is included. A scientificNameID looks\n",
    "like this \"urn:lsid:marinespecies.org:taxname:275730\" with the last digits after\n",
    "the colon being the WoRMS aphia ID. We'll need to go out to WoRMS to grab this\n",
    "information.\n",
    "\n",
    "Create a lookup table of unique scientific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_worms = pd.DataFrame(\n",
    "    columns=['scientificName'],\n",
    "    data=occurrence['scientificName'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the columns that we can grab information from WoRMS including the required scientificNameID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['acceptedname', 'acceptedID', 'scientificNameID', 'kingdom', 'phylum',\n",
    "           'class', 'order', 'family', 'genus', 'scientificNameAuthorship', 'taxonRank']\n",
    "\n",
    "for head in headers:\n",
    "    lut_worms[head] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxonomic lookup using the library [pyworms](https://github.com/iobis/pyworms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in lut_worms.iterrows():\n",
    "    print('Searching for scientific name = %s' % row['scientificName'])\n",
    "    resp = pyworms.aphiaRecordsByMatchNames(row['scientificName'])[0][0]\n",
    "    lut_worms.loc[index, 'acceptedname'] = resp['valid_name']\n",
    "    lut_worms.loc[index, 'acceptedID'] = resp['valid_AphiaID']\n",
    "    lut_worms.loc[index, 'scientificNameID'] = resp['lsid']\n",
    "    lut_worms.loc[index, 'kingdom'] = resp['kingdom']\n",
    "    lut_worms.loc[index, 'phylum'] = resp['phylum']\n",
    "    lut_worms.loc[index, 'class'] = resp['class']\n",
    "    lut_worms.loc[index, 'order'] = resp['order']\n",
    "    lut_worms.loc[index, 'family'] = resp['family']\n",
    "    lut_worms.loc[index, 'genus'] = resp['genus']\n",
    "    lut_worms.loc[index, 'scientificNameAuthorship'] = resp['authority']\n",
    "    lut_worms.loc[index, 'taxonRank'] = resp['rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the lookup table of unique scientific names back with the occurrence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence = pd.merge(occurrence, lut_worms, how='left', on='scientificName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at what we have before we write out the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the columns on scientificName\n",
    "occurrence.sort_values('scientificName', inplace=True)\n",
    "# reorganize column order to be consistent with R example:\n",
    "columns = [\"scientificName\",\"eventID\",\"occurrenceID\",\"occurrenceStatus\",\"acceptedname\",\"acceptedID\",\n",
    "           \"scientificNameID\",\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"scientificNameAuthorship\",\n",
    "           \"taxonRank\"]\n",
    "\n",
    "occurrence.to_csv(\n",
    "    \"data/processed/WBTS_CFIN_2004_2017_occurrence_frompy.csv\",\n",
    "    header=True,\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All done with occurrence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement Or Fact\n",
    "The last file we need to create is the measurement or fact file. For this we need to\n",
    "combine all of the measurements or facts that we want to include making sure to include\n",
    "IDs from the BODC NERC vocabulary where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll manually map the remaining variables to the BODC NERC vocabulary when possible. For now we're mapping the following metadata for each field:\n",
    "\n",
    "1. uri -> URL of the concept page on the NERC VOcabulary Server (NVS)\n",
    "2. unit\n",
    "3. unitId -> URL of the unit ID page on NVS\n",
    "4. accuracy \n",
    "5. type -> measurement type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "\n",
    "vocab_url_prefix = 'http://vocab.nerc.ac.uk/collection/'\n",
    "\n",
    "column_mappings = {\n",
    "    'Net_Type': {'uri': 'L05/current/22/', 'unit': '', 'unitID': '', 'accuracy': '', 'type': ''},\n",
    "    'Mesh_Size': {'uri': 'Q01/current/Q0100015/', 'unit': 'microns', 'unitID': 'P06/current/UMIC/', 'accuracy': '', 'type': ''},\n",
    "    'NET_DEPTH': {'uri': 'P01/current/DXPHPRST/', 'unit': 'm', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Plankton_Net_Area': {'uri': 'Q01/current/Q0100017/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Volume_Filtered': {'uri': 'P25/current/VOL/', 'unit': 'm3', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Sample_Dry_Weight': {'uri': 'P01/current/ODRYBM01/', 'unit': 'g', 'unitID': 'P06/current/UGRM/', 'accuracy': '', 'type': ''},\n",
    "    'DW_G_M_2': {'uri': 'P01/current/ODRYBM01/', 'unit': 'g/m2', 'unitID': 'P06/current/UGMS/', 'accuracy': '', 'type': ''},\n",
    "    'Dilution_Factor': {'uri': '', 'unit': 'ml', 'unitID': 'P06/current/VVML/', 'accuracy': '', 'type': ''},\n",
    "    'TOTAL_DILFACTOR_CFIN': {'uri': '', 'unit': 'ml', 'unitID': 'P06/current/VVML/', 'accuracy': '', 'type': ''},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we'll loop through the mapping list and transform as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_concat = []\n",
    "\n",
    "for current_field in column_mappings:\n",
    "\n",
    "    current_mapping = column_mappings.get(current_field)\n",
    "    \n",
    "    current_df = df[['eventID', current_field, 'time', 'COMMENT', 'Sample_Split']].copy()\n",
    "\n",
    "    #drop units row here\n",
    "    current_df = current_df.iloc[1:, :]\n",
    "\n",
    "    current_df['occurrenceID'] = ''\n",
    "    current_df['measurementType'] = current_mapping.get('type')\n",
    "    current_df['measurementTypeID'] = vocab_url_prefix + current_mapping.get('uri')\n",
    "    current_df['measurementValue'] = current_df[current_field]\n",
    "    current_df['measurementUnit'] = current_mapping.get('unit')\n",
    "    current_df['measurementUnitID'] = vocab_url_prefix + current_mapping.get('unitID') if current_mapping.get('unitID') else ''\n",
    "    current_df['measurementAccuracy'] = current_mapping.get('accuracy')\n",
    "    current_df['measurementDeterminedDate'] = current_df['time']\n",
    "    current_df.drop(\n",
    "        columns=[current_field, 'time'],\n",
    "        inplace=True)\n",
    "    \n",
    "    frames_to_concat.append(current_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all measurements or facts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurementorfact = pd.concat(frames_to_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurementorfact.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add in the remaining fields:\n",
    "\n",
    "1. `measurementMethod`\n",
    "2. `measurementRemark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a constant value as described in the metadata\n",
    "measurementorfact['measurementMethod'] = 'Net used: 0.75 meter diameter single ring or a SEA-GEAR Model 9600 twin-ring, 200Âµm mesh'\n",
    "\n",
    "# this is a constant value, PLUS anything in the 'COMMENT' field for a given occurrence\n",
    "measurementorfact['measurementRemark'] = 'Note: no matching NERC vocabulary URI for sampling device. Comments: ' + df['COMMENT'].astype(str)\n",
    "\n",
    "# drop COMMENT column as we don't need it\n",
    "measurementorfact.drop(columns=['COMMENT'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write measurement or fact file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurementorfact.to_csv('data/processed/WBTS_CFIN_2004_2017_mof_frompy.csv',\n",
    "                         index=False,\n",
    "                         header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
