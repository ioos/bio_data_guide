{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxGSbpxdStLu"
      },
      "source": [
        "# Convert ATN Satellite Telemetry NetCDF to Darwin Core Archive\n",
        "\n",
        "This notebook walks through downloading an example netCDF file from the an Archive package at NCEI and translating it to a Darwin Core Archive compliant package for easy loading and publishing via the Integrated Publishing Toolkit (IPT). The example file follows a specific specification for ATN satellite trajectory observations as documented [here](https://github.com/ioos/ioos-atn-data/blob/main/templates/atn_trajectory_template.cdl). More information about the ATN netCDF specification can be found in the repository <https://github.com/ioos/ioos-atn-data.>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0JvPXasXc432"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from urllib.request import urlopen\n",
        "import urllib.error\n",
        "import xml.etree.ElementTree as ET\n",
        "from owslib import util\n",
        "from owslib.iso import namespaces\n",
        "import xarray as xr\n",
        "import netCDF4\n",
        "import re\n",
        "from jinja2 import Template\n",
        "import codecs\n",
        "import stamina\n",
        "from shapely.geometry import LineString\n",
        "import shapely.wkt\n",
        "from requests_toolbelt.multipart.encoder import MultipartEncoder\n",
        "from dotenv import dotenv_values\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collect the files and accession numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "None",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m namespaces.update({\u001b[33m\"\u001b[39m\u001b[33mgmi\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhttp://www.isotc211.org/2005/gmi\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      3\u001b[39m namespaces.update({\u001b[33m\"\u001b[39m\u001b[33mgml\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhttp://www.opengis.net/gml/3.2\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mnamespaces\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n",
            "\u001b[31mKeyError\u001b[39m: None"
          ]
        }
      ],
      "source": [
        "# Append gmi namespace to namespaces dictionary.\n",
        "namespaces.update({\"gmi\": \"http://www.isotc211.org/2005/gmi\"})\n",
        "namespaces.update({\"gml\": \"http://www.opengis.net/gml/3.2\"})\n",
        "del namespaces[None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "@stamina.retry(on=urllib.error.HTTPError, attempts=3)\n",
        "def openurl(url):\n",
        "    \"\"\"Thin wrapper around urlopen adding stamina.\"\"\"\n",
        "    return urlopen(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create a dataframe to hold the mapping of accession numbers to file names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>accession</th>\n",
              "      <th>start_date</th>\n",
              "      <th>end_date</th>\n",
              "      <th>download_url</th>\n",
              "      <th>arc</th>\n",
              "      <th>xml</th>\n",
              "      <th>file_name</th>\n",
              "      <th>ptt_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Great white shark (Carcharodon carcharias) loc...</td>\n",
              "      <td>0282699</td>\n",
              "      <td>2009-09-23</td>\n",
              "      <td>2009-11-23</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/02...</td>\n",
              "      <td>arc0217</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_45866_great-white-shark_trajectory_2009092...</td>\n",
              "      <td>45866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Great white shark (Carcharodon carcharias) loc...</td>\n",
              "      <td>0282700</td>\n",
              "      <td>2009-09-23</td>\n",
              "      <td>2009-12-13</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/02...</td>\n",
              "      <td>arc0217</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_45869_great-white-shark_trajectory_2009092...</td>\n",
              "      <td>45869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bearded seal (Erignathus barbatus) location da...</td>\n",
              "      <td>0298218</td>\n",
              "      <td>2011-06-18</td>\n",
              "      <td>2012-03-14</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/02...</td>\n",
              "      <td>arc0230</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_38553_bearded-seal_trajectory_20110618-201...</td>\n",
              "      <td>38553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bearded seal (Erignathus barbatus) location da...</td>\n",
              "      <td>0298254</td>\n",
              "      <td>2011-06-16</td>\n",
              "      <td>2012-04-01</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/02...</td>\n",
              "      <td>arc0230</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_39489_bearded-seal_trajectory_20110616-201...</td>\n",
              "      <td>39489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bearded seal (Erignathus barbatus) location da...</td>\n",
              "      <td>0298256</td>\n",
              "      <td>2010-02-13</td>\n",
              "      <td>2012-05-18</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/02...</td>\n",
              "      <td>arc0230</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_64459_bearded-seal_trajectory_20090626-201...</td>\n",
              "      <td>64459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>Spotted seal (Phoca largha) location data from...</td>\n",
              "      <td>0305762</td>\n",
              "      <td>2018-04-20</td>\n",
              "      <td>2018-06-07</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/03...</td>\n",
              "      <td>arc0235</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_174785_spotted-seal_trajectory_20180420-20...</td>\n",
              "      <td>174785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>Spotted seal (Phoca largha) location data from...</td>\n",
              "      <td>0305763</td>\n",
              "      <td>2018-04-14</td>\n",
              "      <td>2018-05-13</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/03...</td>\n",
              "      <td>arc0235</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_174786_spotted-seal_trajectory_20180414-20...</td>\n",
              "      <td>174786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>Spotted seal (Phoca largha) location data from...</td>\n",
              "      <td>0305764</td>\n",
              "      <td>2018-04-09</td>\n",
              "      <td>2018-09-19</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/03...</td>\n",
              "      <td>arc0235</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_174787_spotted-seal_trajectory_20180410-20...</td>\n",
              "      <td>174787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Spotted seal (Phoca largha) location data from...</td>\n",
              "      <td>0305767</td>\n",
              "      <td>2018-04-18</td>\n",
              "      <td>2018-05-28</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/03...</td>\n",
              "      <td>arc0235</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_174790_spotted-seal_trajectory_20180418-20...</td>\n",
              "      <td>174790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>Spotted seal (Phoca largha) location data from...</td>\n",
              "      <td>0305768</td>\n",
              "      <td>2018-04-20</td>\n",
              "      <td>2018-05-25</td>\n",
              "      <td>https://www.ncei.noaa.gov/archive/accession/03...</td>\n",
              "      <td>arc0235</td>\n",
              "      <td>https://www.ncei.noaa.gov/data/oceans/archive/...</td>\n",
              "      <td>atn_174805_spotted-seal_trajectory_20180420-20...</td>\n",
              "      <td>174805</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>289 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title accession  start_date  \\\n",
              "0    Great white shark (Carcharodon carcharias) loc...   0282699  2009-09-23   \n",
              "1    Great white shark (Carcharodon carcharias) loc...   0282700  2009-09-23   \n",
              "2    Bearded seal (Erignathus barbatus) location da...   0298218  2011-06-18   \n",
              "3    Bearded seal (Erignathus barbatus) location da...   0298254  2011-06-16   \n",
              "4    Bearded seal (Erignathus barbatus) location da...   0298256  2010-02-13   \n",
              "..                                                 ...       ...         ...   \n",
              "284  Spotted seal (Phoca largha) location data from...   0305762  2018-04-20   \n",
              "285  Spotted seal (Phoca largha) location data from...   0305763  2018-04-14   \n",
              "286  Spotted seal (Phoca largha) location data from...   0305764  2018-04-09   \n",
              "287  Spotted seal (Phoca largha) location data from...   0305767  2018-04-18   \n",
              "288  Spotted seal (Phoca largha) location data from...   0305768  2018-04-20   \n",
              "\n",
              "       end_date                                       download_url      arc  \\\n",
              "0    2009-11-23  https://www.ncei.noaa.gov/archive/accession/02...  arc0217   \n",
              "1    2009-12-13  https://www.ncei.noaa.gov/archive/accession/02...  arc0217   \n",
              "2    2012-03-14  https://www.ncei.noaa.gov/archive/accession/02...  arc0230   \n",
              "3    2012-04-01  https://www.ncei.noaa.gov/archive/accession/02...  arc0230   \n",
              "4    2012-05-18  https://www.ncei.noaa.gov/archive/accession/02...  arc0230   \n",
              "..          ...                                                ...      ...   \n",
              "284  2018-06-07  https://www.ncei.noaa.gov/archive/accession/03...  arc0235   \n",
              "285  2018-05-13  https://www.ncei.noaa.gov/archive/accession/03...  arc0235   \n",
              "286  2018-09-19  https://www.ncei.noaa.gov/archive/accession/03...  arc0235   \n",
              "287  2018-05-28  https://www.ncei.noaa.gov/archive/accession/03...  arc0235   \n",
              "288  2018-05-25  https://www.ncei.noaa.gov/archive/accession/03...  arc0235   \n",
              "\n",
              "                                                   xml  \\\n",
              "0    https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "1    https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "2    https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "3    https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "4    https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "..                                                 ...   \n",
              "284  https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "285  https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "286  https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "287  https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "288  https://www.ncei.noaa.gov/data/oceans/archive/...   \n",
              "\n",
              "                                             file_name  ptt_id  \n",
              "0    atn_45866_great-white-shark_trajectory_2009092...   45866  \n",
              "1    atn_45869_great-white-shark_trajectory_2009092...   45869  \n",
              "2    atn_38553_bearded-seal_trajectory_20110618-201...   38553  \n",
              "3    atn_39489_bearded-seal_trajectory_20110616-201...   39489  \n",
              "4    atn_64459_bearded-seal_trajectory_20090626-201...   64459  \n",
              "..                                                 ...     ...  \n",
              "284  atn_174785_spotted-seal_trajectory_20180420-20...  174785  \n",
              "285  atn_174786_spotted-seal_trajectory_20180414-20...  174786  \n",
              "286  atn_174787_spotted-seal_trajectory_20180410-20...  174787  \n",
              "287  atn_174790_spotted-seal_trajectory_20180418-20...  174790  \n",
              "288  atn_174805_spotted-seal_trajectory_20180420-20...  174805  \n",
              "\n",
              "[289 rows x 9 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = 'https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.nodc:IOOS-ATN-STP;view=xml;responseType=text/xml'\n",
        "iso = openurl(url)\n",
        "iso_tree = ET.parse(iso)\n",
        "root = iso_tree.getroot()\n",
        "\n",
        "accessions = []\n",
        "# Collect individual accession IDs\n",
        "for MD_keywords in root.iterfind(\".//gmd:descriptiveKeywords/gmd:MD_Keywords\", namespaces):\n",
        "  for thesaurus_name in MD_keywords.iterfind(\".//gmd:thesaurusName/gmd:CI_Citation/gmd:title/gco:CharacterString\",namespaces,):\n",
        "    if thesaurus_name.text == \"NCEI ACCESSION NUMBER\":\n",
        "            for acce_no in MD_keywords.iterfind(\".//gmd:keyword/gmx:Anchor\", namespaces):\n",
        "              accessions.append(acce_no.text)\n",
        "\n",
        "df_map = pd.DataFrame()\n",
        "\n",
        "for acc in accessions:\n",
        "  ## There are a couple endpoints we can use\n",
        "  #\n",
        "  # https://www.ncei.noaa.gov/data/oceans/ncei/archive/metadata/approved/granule/{acc}.xml\n",
        "  # https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.nodc:{acc};view=xml\n",
        "  # http://www.ncei.noaa.gov/metadata/granule/geoportal/rest/metadata/item/IOOS-ATN-STP.{acc}/xml\n",
        "  #\n",
        "  url = f'https://www.ncei.noaa.gov/data/oceans/ncei/archive/metadata/approved/granule/{acc}.xml'\n",
        "  iso = openurl(url)\n",
        "  iso_tree = ET.parse(iso)\n",
        "  root = iso_tree.getroot()\n",
        "\n",
        "  # Collect terms of interest.\n",
        "  title = pd.DataFrame({\n",
        "      'title': [root.find(\".//gmd:title/gco:CharacterString\", namespaces).text],\n",
        "      'accession': acc,\n",
        "      'start_date': [root.find('.//gml:TimePeriod/gml:beginPosition',namespaces).text],\n",
        "      'end_date': [root.find('.//gml:TimePeriod/gml:endPosition',namespaces).text],\n",
        "      })\n",
        "  \n",
        "  for CI_OnlineResource in root.iterfind('.//gmd:onlineResource/gmd:CI_OnlineResource',namespaces):\n",
        "\n",
        "    if CI_OnlineResource.find('.//gmd:CI_OnLineFunctionCode',namespaces).text == 'download':\n",
        "\n",
        "      data_url = CI_OnlineResource.find('.//gmd:URL',namespaces).text+\"/data/0-data/\"\n",
        "\n",
        "      title['download_url'] = data_url\n",
        "\n",
        "  for CI_OnlineResource in root.iterfind('.//gmd:onLine/gmd:CI_OnlineResource',namespaces):\n",
        "\n",
        "    if CI_OnlineResource.find('.//gmd:protocol/gco:CharacterString',namespaces).text == 'FTP':\n",
        "\n",
        "      string = CI_OnlineResource.find('.//gmd:linkage/gmd:URL',namespaces).text\n",
        "      arc = re.search(\"(arc[0-9]{1,4})\",string)\n",
        "\n",
        "      if arc:\n",
        "        title['arc'] = [arc.group()]\n",
        "        xml_manifest = f'https://www.ncei.noaa.gov/data/oceans/archive/{arc.group()}/{acc}/{acc}.1.1.xml'\n",
        "\n",
        "        title['xml'] = [xml_manifest]\n",
        "\n",
        "        iso_mani = openurl(xml_manifest)\n",
        "        iso_mani_tree = ET.parse(iso_mani)\n",
        "        root_mani = iso_mani_tree.getroot()\n",
        "\n",
        "        for path in root_mani.iterfind('.//path'):\n",
        "          fpath = path.text\n",
        "          fname = re.search(\"(data/0-data/atn_.*)\",fpath)\n",
        "          \n",
        "          if fname:\n",
        "            title['file_name'] = fname.group().split(\"/\")[-1]\n",
        "\n",
        "  df_map = pd.concat([df_map, title],ignore_index=True)\n",
        "\n",
        "df_map['ptt_id'] = df_map['title'].str.extract(r'.*ptt ([0-9]{3,7}) .*')\n",
        "\n",
        "df_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtUi2y_3cwZq"
      },
      "source": [
        "## Create a function to recursively download files\n",
        "\n",
        "think about pulling from NCEI accessions instead of downloading from WAF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "puONkrPNceM5"
      },
      "outputs": [],
      "source": [
        "def recursive_wget(url, output_dir):\n",
        "    \"\"\"\n",
        "    Recursively downloads files from a given URL to a specified output directory,\n",
        "    mirroring the directory structure of the website.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to start downloading from.\n",
        "        output_dir (str): The local directory to save files to.\n",
        "    \"\"\"\n",
        "    print(f\"Accessing: {url}\")\n",
        "    try:\n",
        "        # --- Create the output directory if it doesn't exist ---\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "            print(f\"Created directory: {output_dir}\")\n",
        "\n",
        "        # --- Send a GET request and parse the HTML ---\n",
        "        response = requests.get(url)\n",
        "        # Raise an exception for bad status codes (like 404 Not Found)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # --- Find all links on the page ---\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "\n",
        "            # --- Skip invalid or parent directory links ---\n",
        "            if not href or href.startswith('?') or href.startswith('/') or '..' in href:\n",
        "                continue\n",
        "\n",
        "            # --- Construct the full, absolute URL for the link ---\n",
        "            absolute_url = urljoin(url, href)\n",
        "\n",
        "            # Get the path component of the URL to create local directories/files\n",
        "            path = urlparse(absolute_url).path\n",
        "            # Create a valid local path from the last part of the URL path\n",
        "            local_path = os.path.join(output_dir, os.path.basename(path))\n",
        "\n",
        "            # --- If the link points to a directory, recurse into it ---\n",
        "            if href.endswith('/'):\n",
        "                print(f\"\\nEntering directory: {absolute_url}\")\n",
        "                # Call the function again for the new directory\n",
        "                recursive_wget(absolute_url, local_path)\n",
        "            # --- If the link points to a file, download it ---\n",
        "            else:\n",
        "                download_file(absolute_url, local_path)\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error accessing URL {url}: {e}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error accessing URL {url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "def download_file(url, local_path):\n",
        "    \"\"\"\n",
        "    Downloads a single file from a URL and saves it to a local path.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the file to download.\n",
        "        local_path (str): The local path where the file will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"  Downloading file: {os.path.basename(local_path)}\")\n",
        "        # Use stream=True to efficiently download large files\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            # Open the file in binary write mode\n",
        "            with open(local_path, 'wb') as f:\n",
        "                # Write the file in chunks\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "        # print(f\"  Successfully downloaded {os.path.basename(local_path)}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  Failed to download {url}: {e}\")\n",
        "    except IOError as e:\n",
        "        print(f\"  Failed to write file {local_path}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCEbQSINcjQm"
      },
      "source": [
        "## Execute download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W39rehjcioS",
        "outputId": "b1730911-fc6e-481a-c079-977dbaf17f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Recursive Download ---\n",
            "Source URL: https://www.ncei.noaa.gov/data/oceans/ioos/atn/\n",
            "Local Directory: data/src/\n",
            "\n",
            "Accessing: https://www.ncei.noaa.gov/data/oceans/ioos/atn/\n",
            "  Downloading file: ACCESSION_UPDATE_LOG.TXT\n",
            "\n",
            "Entering directory: https://www.ncei.noaa.gov/data/oceans/ioos/atn/california_state_university_long_beach/\n",
            "Accessing: https://www.ncei.noaa.gov/data/oceans/ioos/atn/california_state_university_long_beach/\n",
            "  Downloading file: atn_45866_great-white-shark_trajectory_20090923-20091123.nc\n",
            "  Downloading file: atn_45869_great-white-shark_trajectory_20090923-20091213.nc\n",
            "\n",
            "Entering directory: https://www.ncei.noaa.gov/data/oceans/ioos/atn/cascadia_research_collective/\n",
            "Accessing: https://www.ncei.noaa.gov/data/oceans/ioos/atn/cascadia_research_collective/\n",
            "  Downloading file: atn_53631_false-killer-whale_trajectory_20100927-20101001.nc\n",
            "  Downloading file: atn_53644_false-killer-whale_trajectory_20100927-20101118.nc\n",
            "  Downloading file: atn_53652_false-killer-whale_trajectory_20101023-20101213.nc\n",
            "  Downloading file: atn_77250_false-killer-whale_trajectory_20070816-20070830.nc\n",
            "  Downloading file: atn_77250_false-killer-whale_trajectory_20080422-20080513.nc\n",
            "  Downloading file: atn_77251_false-killer-whale_trajectory_20070816-20070818.nc\n",
            "  Downloading file: atn_77252_false-killer-whale_trajectory_20070816-20070917.nc\n",
            "  Downloading file: atn_77252_false-killer-whale_trajectory_20080727-20080915.nc\n",
            "  Downloading file: atn_77253_false-killer-whale_trajectory_20080727-20080824.nc\n",
            "  Downloading file: atn_85567_false-killer-whale_trajectory_20081211-20090203.nc\n",
            "  Downloading file: atn_85568_short-finned-pilot-whale_trajectory_20080705-20080802.nc\n",
            "  Downloading file: atn_85586_false-killer-whale_trajectory_20080717-20080821.nc\n",
            "  Downloading file: atn_85587_false-killer-whale_trajectory_20080717-20080806.nc\n",
            "  Downloading file: atn_85588_false-killer-whale_trajectory_20080717-20080911.nc\n",
            "  Downloading file: atn_85589_false-killer-whale_trajectory_20080717-20080723.nc\n",
            "  Downloading file: atn_85590_false-killer-whale_trajectory_20080717-20081001.nc\n",
            "  Downloading file: atn_94793_false-killer-whale_trajectory_20091018-20091227.nc\n",
            "  Downloading file: atn_94804_false-killer-whale_trajectory_20101016-20101029.nc\n",
            "  Downloading file: atn_94808_false-killer-whale_trajectory_20091014-20091025.nc\n",
            "  Downloading file: atn_94811_false-killer-whale_trajectory_20091006-20100103.nc\n",
            "  Downloading file: atn_94812_false-killer-whale_trajectory_20091015-20100122.nc\n",
            "  Downloading file: atn_94813_false-killer-whale_trajectory_20091017-20091117.nc\n",
            "  Downloading file: atn_94818_false-killer-whale_trajectory_20091211-20100320.nc\n",
            "  Downloading file: atn_94819_false-killer-whale_trajectory_20091211-20091227.nc\n",
            "  Downloading file: atn_94820_false-killer-whale_trajectory_20091219-20100403.nc\n",
            "  Downloading file: atn_94821_blainvilles-beaked-whale_trajectory_20091221-20100111.nc\n",
            "  Downloading file: atn_94822_false-killer-whale_trajectory_20091219-20100129.nc\n",
            "  Downloading file: atn_98364_false-killer-whale_trajectory_20131023-20140223.nc\n",
            "  Downloading file: atn_98365_false-killer-whale_trajectory_20131023-20131107.nc\n",
            "\n",
            "Entering directory: https://www.ncei.noaa.gov/data/oceans/ioos/atn/marine_mammal_laboratory_noaa_alaska_fisheries_science_center/\n",
            "Accessing: https://www.ncei.noaa.gov/data/oceans/ioos/atn/marine_mammal_laboratory_noaa_alaska_fisheries_science_center/\n",
            "  Downloading file: atn_37515_spotted-seal_trajectory_20100522-20110423.nc\n",
            "  Downloading file: atn_37909_spotted-seal_trajectory_20140427-20140921.nc\n",
            "  Downloading file: atn_38549_spotted-seal_trajectory_20140429-20140613.nc\n",
            "  Downloading file: atn_39482_spotted-seal_trajectory_20140427-20141022.nc\n",
            "  Downloading file: atn_39486_ribbon-seal_trajectory_20100517-20100917.nc\n",
            "  Downloading file: atn_39490_ribbon-seal_trajectory_20140426-20140526.nc\n",
            "  Downloading file: atn_39491_ribbon-seal_trajectory_20100516-20100518.nc\n",
            "  Downloading file: atn_39496_ribbon-seal_trajectory_20100523-20101216.nc\n",
            "  Downloading file: atn_39498_ribbon-seal_trajectory_20140427-20140509.nc\n",
            "  Downloading file: atn_39499_ribbon-seal_trajectory_20140419-20140428.nc\n",
            "  Downloading file: atn_40857_ribbon-seal_trajectory_20100523-20110408.nc\n",
            "  Downloading file: atn_40858_ribbon-seal_trajectory_20140421-20140426.nc\n",
            "  Downloading file: atn_40862_ribbon-seal_trajectory_20140420-20140428.nc\n",
            "  Downloading file: atn_57998_ribbon-seal_trajectory_20050525-20060508.nc\n",
            "  Downloading file: atn_57999_spotted-seal_trajectory_20050925-20060521.nc\n",
            "  Downloading file: atn_58000_ribbon-seal_trajectory_20050601-20060403.nc\n",
            "  Downloading file: atn_58001_spotted-seal_trajectory_20050926-20051215.nc\n",
            "  Downloading file: atn_58002_spotted-seal_trajectory_20050926-20051223.nc\n",
            "  Downloading file: atn_58003_spotted-seal_trajectory_20050928-20060428.nc\n",
            "  Downloading file: atn_58005_ribbon-seal_trajectory_20050601-20060508.nc\n",
            "  Downloading file: atn_58006_ribbon-seal_trajectory_20050602-20060220.nc\n",
            "  Downloading file: atn_58007_ribbon-seal_trajectory_20050602-20050603.nc\n",
            "  Downloading file: atn_58008_ribbon-seal_trajectory_20050602-20060508.nc\n",
            "  Downloading file: atn_58009_ribbon-seal_trajectory_20050606-20051215.nc\n",
            "  Downloading file: atn_58010_ribbon-seal_trajectory_20050607-20051213.nc\n",
            "  Downloading file: atn_58011_ribbon-seal_trajectory_20050602-20050604.nc\n",
            "  Downloading file: atn_58012_ribbon-seal_trajectory_20050607-20060421.nc\n",
            "  Downloading file: atn_58014_spotted-seal_trajectory_20050929-20060329.nc\n",
            "  Downloading file: atn_62755_spotted-seal_trajectory_20060424-20060506.nc\n",
            "  Downloading file: atn_62756_spotted-seal_trajectory_20060427-20060513.nc\n",
            "  Downloading file: atn_64451_spotted-seal_trajectory_20090518-20090612.nc\n",
            "  Downloading file: atn_64452_spotted-seal_trajectory_20090528-20090612.nc\n",
            "  Downloading file: atn_64453_ribbon-seal_trajectory_20090606-20090612.nc\n",
            "  Downloading file: atn_64455_spotted-seal_trajectory_20090517-20100430.nc\n",
            "  Downloading file: atn_64457_ribbon-seal_trajectory_20090603-20090613.nc\n",
            "  Downloading file: atn_64460_spotted-seal_trajectory_20090518-20090618.nc\n",
            "  Downloading file: atn_64461_ribbon-seal_trajectory_20090607-20100524.nc\n",
            "  Downloading file: atn_64463_spotted-seal_trajectory_20090528-20090620.nc\n",
            "  Downloading file: atn_64464_ribbon-seal_trajectory_20090531-20100506.nc\n",
            "  Downloading file: atn_64465_ribbon-seal_trajectory_20090602-20090607.nc\n",
            "  Downloading file: atn_64467_ribbon-seal_trajectory_20090606-20100531.nc\n",
            "  Downloading file: atn_64469_ribbon-seal_trajectory_20090607-20110526.nc\n",
            "  Downloading file: atn_64471_ribbon-seal_trajectory_20090605-20100522.nc\n",
            "  Downloading file: atn_64472_ribbon-seal_trajectory_20090602-20090607.nc\n",
            "  Downloading file: atn_64473_spotted-seal_trajectory_20090518-20090618.nc\n",
            "  Downloading file: atn_64476_ribbon-seal_trajectory_20090528-20100603.nc\n",
            "  Downloading file: atn_64477_spotted-seal_trajectory_20090528-20100422.nc\n",
            "  Downloading file: atn_64478_ribbon-seal_trajectory_20090531-20091230.nc\n",
            "  Downloading file: atn_64479_ribbon-seal_trajectory_20090607-20100126.nc\n",
            "  Downloading file: atn_64481_ribbon-seal_trajectory_20090604-20091221.nc\n",
            "  Downloading file: atn_64482_ribbon-seal_trajectory_20090603-20100514.nc\n",
            "  Downloading file: atn_64483_ribbon-seal_trajectory_20100505-20100928.nc\n",
            "  Downloading file: atn_64484_ribbon-seal_trajectory_20090530-20100326.nc\n",
            "  Downloading file: atn_64485_spotted-seal_trajectory_20090603-20091023.nc\n",
            "  Downloading file: atn_64486_ribbon-seal_trajectory_20090601-20100426.nc\n",
            "  Downloading file: atn_64488_spotted-seal_trajectory_20090606-20100310.nc\n",
            "  Downloading file: atn_64489_ribbon-seal_trajectory_20090529-20100502.nc\n",
            "  Downloading file: atn_64490_ribbon-seal_trajectory_20090531-20100105.nc\n",
            "  Downloading file: atn_64491_ribbon-seal_trajectory_20090529-20090617.nc\n",
            "  Downloading file: atn_64492_ribbon-seal_trajectory_20090602-20100116.nc\n",
            "  Downloading file: atn_64706_ribbon-seal_trajectory_20060427-20060520.nc\n",
            "  Downloading file: atn_64707_ribbon-seal_trajectory_20060502-20060505.nc\n",
            "  Downloading file: atn_64708_ribbon-seal_trajectory_20060503-20060527.nc\n",
            "  Downloading file: atn_64709_ribbon-seal_trajectory_20060502-20060511.nc\n",
            "  Downloading file: atn_64710_ribbon-seal_trajectory_20060503-20060520.nc\n",
            "  Downloading file: atn_64712_spotted-seal_trajectory_20060503-20060506.nc\n",
            "  Downloading file: atn_64713_spotted-seal_trajectory_20060503-20060518.nc\n",
            "  Downloading file: atn_64714_spotted-seal_trajectory_20060501-20060513.nc\n",
            "  Downloading file: atn_64715_ribbon-seal_trajectory_20060503-20060527.nc\n",
            "  Downloading file: atn_64717_spotted-seal_trajectory_20060503-20060516.nc\n",
            "  Downloading file: atn_64899_ribbon-seal_trajectory_20090604-20090612.nc\n",
            "  Downloading file: atn_65922_ribbon-seal_trajectory_20060506-20060508.nc\n",
            "  Downloading file: atn_65924_ribbon-seal_trajectory_20060502-20060618.nc\n",
            "  Downloading file: atn_65925_ribbon-seal_trajectory_20060505-20060525.nc\n",
            "  Downloading file: atn_65926_ribbon-seal_trajectory_20060503-20060516.nc\n",
            "  Downloading file: atn_65927_ribbon-seal_trajectory_20070522-20080107.nc\n",
            "  Downloading file: atn_65928_ribbon-seal_trajectory_20070517-20080402.nc\n",
            "  Downloading file: atn_65931_ribbon-seal_trajectory_20070522-20080130.nc\n",
            "  Downloading file: atn_65932_spotted-seal_trajectory_20060506-20060620.nc\n",
            "  Downloading file: atn_65933_spotted-seal_trajectory_20060427-20060511.nc\n",
            "  Downloading file: atn_66928_ribbon-seal_trajectory_20140426-20140518.nc\n",
            "  Downloading file: atn_66949_ribbon-seal_trajectory_20140415-20140503.nc\n",
            "  Downloading file: atn_66973_spotted-seal_trajectory_20140427-20141222.nc\n",
            "  Downloading file: atn_66978_spotted-seal_trajectory_20140429-20141201.nc\n",
            "  Downloading file: atn_66989_ribbon-seal_trajectory_20140428-20140518.nc\n",
            "  Downloading file: atn_66990_ribbon-seal_trajectory_20140428-20140501.nc\n",
            "  Downloading file: atn_67000_spotted-seal_trajectory_20140429-20140721.nc\n",
            "  Downloading file: atn_67003_ribbon-seal_trajectory_20140428-20140519.nc\n",
            "  Downloading file: atn_67026_ribbon-seal_trajectory_20140427-20141108.nc\n",
            "  Downloading file: atn_74629_ribbon-seal_trajectory_20070529-20080404.nc\n",
            "  Downloading file: atn_74631_spotted-seal_trajectory_20090606-20100504.nc\n",
            "  Downloading file: atn_74633_spotted-seal_trajectory_20090518-20090731.nc\n",
            "  Downloading file: atn_74634_ribbon-seal_trajectory_20090602-20100222.nc\n",
            "  Downloading file: atn_74635_spotted-seal_trajectory_20090523-20091028.nc\n",
            "  Downloading file: atn_74636_ribbon-seal_trajectory_20090528-20100209.nc\n",
            "  Downloading file: atn_74637_spotted-seal_trajectory_20100506-20110410.nc\n",
            "  Downloading file: atn_74638_ribbon-seal_trajectory_20100510-20100605.nc\n",
            "  Downloading file: atn_74639_ribbon-seal_trajectory_20070524-20070902.nc\n",
            "  Downloading file: atn_74640_ribbon-seal_trajectory_20070529-20071022.nc\n",
            "  Downloading file: atn_74641_ribbon-seal_trajectory_20070527-20071103.nc\n",
            "  Downloading file: atn_74642_spotted-seal_trajectory_20070508-20071123.nc\n",
            "  Downloading file: atn_74643_ribbon-seal_trajectory_20070525-20080502.nc\n",
            "  Downloading file: atn_74644_spotted-seal_trajectory_20070507-20080303.nc\n",
            "  Downloading file: atn_74645_ribbon-seal_trajectory_20070524-20070605.nc\n",
            "  Downloading file: atn_74646_ribbon-seal_trajectory_20070528-20100529.nc\n",
            "  Downloading file: atn_74647_ribbon-seal_trajectory_20070520-20080612.nc\n",
            "  Downloading file: atn_74648_ribbon-seal_trajectory_20070523-20080402.nc\n",
            "  Downloading file: atn_74649_ribbon-seal_trajectory_20070524-20070614.nc\n",
            "  Downloading file: atn_74650_ribbon-seal_trajectory_20070525-20080617.nc\n",
            "  Downloading file: atn_74651_ribbon-seal_trajectory_20070516-20080613.nc\n",
            "  Downloading file: atn_74652_ribbon-seal_trajectory_20070523-20080502.nc\n",
            "  Downloading file: atn_74653_ribbon-seal_trajectory_20070518-20100426.nc\n",
            "  Downloading file: atn_74654_spotted-seal_trajectory_20070520-20070713.nc\n",
            "  Downloading file: atn_74655_spotted-seal_trajectory_20070526-20080619.nc\n",
            "  Downloading file: atn_74656_spotted-seal_trajectory_20070524-20070613.nc\n",
            "  Downloading file: atn_74657_ribbon-seal_trajectory_20070518-20070608.nc\n",
            "  Downloading file: atn_74658_spotted-seal_trajectory_20070520-20070613.nc\n",
            "  Downloading file: atn_74662_ribbon-seal_trajectory_20070527-20070626.nc\n",
            "  Downloading file: atn_74664_ribbon-seal_trajectory_20070518-20080621.nc\n",
            "  Downloading file: atn_74665_ribbon-seal_trajectory_20070522-20080614.nc\n",
            "  Downloading file: atn_74666_spotted-seal_trajectory_20070524-20070629.nc\n",
            "  Downloading file: atn_74670_ribbon-seal_trajectory_20090609-20100607.nc\n",
            "  Downloading file: atn_74672_ribbon-seal_trajectory_20070527-20080622.nc\n",
            "  Downloading file: atn_74673_ribbon-seal_trajectory_20070510-20070611.nc\n",
            "  Downloading file: atn_74674_spotted-seal_trajectory_20070521-20090601.nc\n",
            "  Downloading file: atn_74677_ribbon-seal_trajectory_20070523-20090522.nc\n",
            "  Downloading file: atn_74679_ribbon-seal_trajectory_20070523-20070613.nc\n",
            "  Downloading file: atn_74681_spotted-seal_trajectory_20070424-20070701.nc\n",
            "  Downloading file: atn_74682_ribbon-seal_trajectory_20070526-20080610.nc\n",
            "  Downloading file: atn_74685_spotted-seal_trajectory_20070424-20070625.nc\n",
            "  Downloading file: atn_74688_ribbon-seal_trajectory_20070527-20071115.nc\n",
            "  Downloading file: atn_74689_spotted-seal_trajectory_20070507-20070901.nc\n",
            "  Downloading file: atn_74690_ribbon-seal_trajectory_20070505-20070521.nc\n",
            "  Downloading file: atn_74693_spotted-seal_trajectory_20070526-20080301.nc\n",
            "  Downloading file: atn_74698_ribbon-seal_trajectory_20070522-20070605.nc\n",
            "  Downloading file: atn_83881_ribbon-seal_trajectory_20080429-20080514.nc\n",
            "  Downloading file: atn_83885_spotted-seal_trajectory_20090606-20100612.nc\n",
            "  Downloading file: atn_83901_spotted-seal_trajectory_20090531-20090707.nc\n",
            "  Downloading file: atn_83902_ribbon-seal_trajectory_20090602-20100101.nc\n",
            "  Downloading file: atn_83905_ribbon-seal_trajectory_20090602-20090607.nc\n",
            "  Downloading file: atn_83908_spotted-seal_trajectory_20090414-20100507.nc\n",
            "  Downloading file: atn_83912_ribbon-seal_trajectory_20090530-20100520.nc\n",
            "  Downloading file: atn_83913_spotted-seal_trajectory_20090606-20090902.nc\n",
            "  Downloading file: atn_83916_spotted-seal_trajectory_20090529-20090707.nc\n",
            "  Downloading file: atn_83918_ribbon-seal_trajectory_20090606-20100308.nc\n",
            "  Downloading file: atn_83922_spotted-seal_trajectory_20090414-20120528.nc\n",
            "  Downloading file: atn_83923_ribbon-seal_trajectory_20090604-20090609.nc\n",
            "  Downloading file: atn_83926_spotted-seal_trajectory_20090414-20100512.nc\n",
            "  Downloading file: atn_85853_ribbon-seal_trajectory_20090605-20100318.nc\n",
            "  Downloading file: atn_85854_ribbon-seal_trajectory_20090607-20090926.nc\n",
            "  Downloading file: atn_85855_spotted-seal_trajectory_20100508-20101031.nc\n",
            "  Downloading file: atn_85857_spotted-seal_trajectory_20090607-20100130.nc\n",
            "  Downloading file: atn_85858_spotted-seal_trajectory_20100505-20110214.nc\n",
            "  Downloading file: atn_85859_spotted-seal_trajectory_20090531-20100201.nc\n",
            "  Downloading file: atn_85860_ribbon-seal_trajectory_20090604-20090801.nc\n",
            "  Downloading file: atn_85861_ribbon-seal_trajectory_20090602-20090925.nc\n",
            "  Downloading file: atn_85862_spotted-seal_trajectory_20090519-20090810.nc\n",
            "  Downloading file: atn_85863_ribbon-seal_trajectory_20090602-20100127.nc\n",
            "  Downloading file: atn_85864_spotted-seal_trajectory_20090519-20091216.nc\n",
            "  Downloading file: atn_85865_ribbon-seal_trajectory_20100517-20110204.nc\n",
            "  Downloading file: atn_85866_spotted-seal_trajectory_20090529-20090704.nc\n",
            "  Downloading file: atn_85867_spotted-seal_trajectory_20090609-20090929.nc\n",
            "  Downloading file: atn_85868_ribbon-seal_trajectory_20090603-20100223.nc\n",
            "  Downloading file: atn_85869_spotted-seal_trajectory_20100506-20110108.nc\n",
            "  Downloading file: atn_85870_ribbon-seal_trajectory_20090609-20091228.nc\n",
            "  Downloading file: atn_85871_spotted-seal_trajectory_20090523-20100228.nc\n",
            "  Downloading file: atn_85872_ribbon-seal_trajectory_20090607-20091204.nc\n",
            "  Downloading file: atn_85873_ribbon-seal_trajectory_20100503-20100601.nc\n",
            "  Downloading file: atn_85874_ribbon-seal_trajectory_20090602-20090812.nc\n",
            "  Downloading file: atn_85875_spotted-seal_trajectory_20090531-20090921.nc\n",
            "  Downloading file: atn_85876_ribbon-seal_trajectory_20090527-20100325.nc\n",
            "  Downloading file: atn_85877_spotted-seal_trajectory_20100506-20100626.nc\n",
            "  Downloading file: atn_85878_spotted-seal_trajectory_20090608-20100320.nc\n",
            "  Downloading file: atn_85879_ribbon-seal_trajectory_20100509-20100630.nc\n",
            "  Downloading file: atn_85880_ribbon-seal_trajectory_20100514-20101224.nc\n",
            "  Downloading file: atn_85881_ribbon-seal_trajectory_20090604-20100330.nc\n",
            "  Downloading file: atn_99277_ribbon-seal_trajectory_20140412-20140503.nc\n",
            "  Downloading file: atn_99279_ribbon-seal_trajectory_20140426-20140513.nc\n",
            "  Downloading file: atn_99280_ribbon-seal_trajectory_20140428-20140506.nc\n",
            "  Downloading file: atn_99282_ribbon-seal_trajectory_20140428-20140514.nc\n",
            "  Downloading file: atn_99283_ribbon-seal_trajectory_20100509-20110518.nc\n",
            "  Downloading file: atn_99288_spotted-seal_trajectory_20100522-20100901.nc\n",
            "  Downloading file: atn_99292_ribbon-seal_trajectory_20100523-20100612.nc\n",
            "  Downloading file: atn_99293_ribbon-seal_trajectory_20100509-20100801.nc\n",
            "  Downloading file: atn_99295_spotted-seal_trajectory_20100523-20110730.nc\n",
            "  Downloading file: atn_99297_ribbon-seal_trajectory_20100515-20100530.nc\n",
            "  Downloading file: atn_99298_ribbon-seal_trajectory_20100517-20120530.nc\n",
            "  Downloading file: atn_99299_ribbon-seal_trajectory_20100505-20100605.nc\n",
            "  Downloading file: atn_99300_ribbon-seal_trajectory_20100517-20110513.nc\n",
            "  Downloading file: atn_99301_ribbon-seal_trajectory_20100523-20110201.nc\n",
            "  Downloading file: atn_99305_spotted-seal_trajectory_20100522-20100730.nc\n",
            "  Downloading file: atn_99306_ribbon-seal_trajectory_20100523-20100530.nc\n",
            "  Downloading file: atn_99308_ribbon-seal_trajectory_20100517-20110607.nc\n",
            "  Downloading file: atn_99309_ribbon-seal_trajectory_20100514-20101215.nc\n",
            "  Downloading file: atn_99312_ribbon-seal_trajectory_20100516-20100718.nc\n",
            "  Downloading file: atn_131373_ribbon-seal_trajectory_20140428-20141213.nc\n",
            "  Downloading file: atn_137487_ribbon-seal_trajectory_20140412-20140413.nc\n",
            "  Downloading file: atn_137490_spotted-seal_trajectory_20160414-20160414.nc\n",
            "  Downloading file: atn_137491_spotted-seal_trajectory_20180418-20180526.nc\n",
            "  Downloading file: atn_137494_ribbon-seal_trajectory_20140426-20140426.nc\n",
            "  Downloading file: atn_137495_ribbon-seal_trajectory_20140426-20140427.nc\n",
            "  Downloading file: atn_137495_spotted-seal_trajectory_20170809-20180607.nc\n",
            "  Downloading file: atn_137497_spotted-seal_trajectory_20160412-20170721.nc\n",
            "  Downloading file: atn_137500_ribbon-seal_trajectory_20140426-20140427.nc\n",
            "  Downloading file: atn_137506_ribbon-seal_trajectory_20140421-20140421.nc\n",
            "  Downloading file: atn_137514_ribbon-seal_trajectory_20140419-20140420.nc\n",
            "  Downloading file: atn_137516_ribbon-seal_trajectory_20140420-20140420.nc\n",
            "  Downloading file: atn_141929_spotted-seal_trajectory_20160425-20160514.nc\n",
            "  Downloading file: atn_143947_spotted-seal_trajectory_20160414-20160623.nc\n",
            "  Downloading file: atn_143956_spotted-seal_trajectory_20180410-20180901.nc\n",
            "  Downloading file: atn_143966_ribbon-seal_trajectory_20160416-20160430.nc\n",
            "  Downloading file: atn_143979_spotted-seal_trajectory_20160414-20160422.nc\n",
            "  Downloading file: atn_143982_ribbon-seal_trajectory_20160423-20160510.nc\n",
            "  Downloading file: atn_143991_spotted-seal_trajectory_20160415-20160509.nc\n",
            "  Downloading file: atn_143994_spotted-seal_trajectory_20160423-20160516.nc\n",
            "  Downloading file: atn_144001_ribbon-seal_trajectory_20160413-20160427.nc\n",
            "  Downloading file: atn_144004_ribbon-seal_trajectory_20160421-20160424.nc\n",
            "  Downloading file: atn_144006_spotted-seal_trajectory_20160412-20160420.nc\n",
            "  Downloading file: atn_144009_spotted-seal_trajectory_20160414-20160510.nc\n",
            "  Downloading file: atn_144015_ribbon-seal_trajectory_20160416-20160418.nc\n",
            "  Downloading file: atn_144017_ribbon-seal_trajectory_20160425-20160512.nc\n",
            "  Downloading file: atn_160948_ribbon-seal_trajectory_20160421-20170519.nc\n",
            "  Downloading file: atn_160951_spotted-seal_trajectory_20160817-20170828.nc\n",
            "  Downloading file: atn_160954_spotted-seal_trajectory_20160814-20161219.nc\n",
            "  Downloading file: atn_160955_ribbon-seal_trajectory_20160413-20170605.nc\n",
            "  Downloading file: atn_160956_spotted-seal_trajectory_20160415-20160622.nc\n",
            "  Downloading file: atn_160957_spotted-seal_trajectory_20170816-20171125.nc\n",
            "  Downloading file: atn_160960_spotted-seal_trajectory_20160817-20170729.nc\n",
            "  Downloading file: atn_160965_ribbon-seal_trajectory_20160416-20160725.nc\n",
            "  Downloading file: atn_160966_spotted-seal_trajectory_20160423-20170120.nc\n",
            "  Downloading file: atn_160968_ribbon-seal_trajectory_20160425-20160726.nc\n",
            "  Downloading file: atn_160969_ribbon-seal_trajectory_20160416-20161230.nc\n",
            "  Downloading file: atn_160972_spotted-seal_trajectory_20170816-20171121.nc\n",
            "  Downloading file: atn_160973_ribbon-seal_trajectory_20160423-20170615.nc\n",
            "  Downloading file: atn_160975_ribbon-seal_trajectory_20160426-20170513.nc\n",
            "  Downloading file: atn_160977_spotted-seal_trajectory_20160425-20161009.nc\n",
            "  Downloading file: atn_164869_spotted-seal_trajectory_20180420-20180514.nc\n",
            "  Downloading file: atn_174785_spotted-seal_trajectory_20180420-20180605.nc\n",
            "  Downloading file: atn_174786_spotted-seal_trajectory_20180414-20180511.nc\n",
            "  Downloading file: atn_174787_spotted-seal_trajectory_20180410-20180610.nc\n",
            "  Downloading file: atn_174790_spotted-seal_trajectory_20180418-20180527.nc\n",
            "  Downloading file: atn_174805_spotted-seal_trajectory_20180420-20180523.nc\n",
            "  Downloading file: atn_174821_spotted-seal_trajectory_20180420-20180618.nc\n",
            "  Downloading file: atn_174822_spotted-seal_trajectory_20180414-20181103.nc\n",
            "\n",
            "Entering directory: https://www.ncei.noaa.gov/data/oceans/ioos/atn/noaa_alaska_fisheries_science_center/\n",
            "Accessing: https://www.ncei.noaa.gov/data/oceans/ioos/atn/noaa_alaska_fisheries_science_center/\n",
            "  Downloading file: atn_38553_bearded-seal_trajectory_20110618-20120314.nc\n",
            "  Downloading file: atn_39489_bearded-seal_trajectory_20110616-20120401.nc\n",
            "  Downloading file: atn_64459_bearded-seal_trajectory_20090626-20120518.nc\n",
            "  Downloading file: atn_64462_bearded-seal_trajectory_20090623-20120612.nc\n",
            "  Downloading file: atn_66971_bearded-seal_trajectory_20110617-20120131.nc\n",
            "  Downloading file: atn_66983_bearded-seal_trajectory_20110618-20140626.nc\n",
            "  Downloading file: atn_67004_bearded-seal_trajectory_20120704-20130618.nc\n",
            "  Downloading file: atn_67007_bearded-seal_trajectory_20110616-20120813.nc\n",
            "  Downloading file: atn_74626_bearded-seal_trajectory_20090625-20100128.nc\n",
            "  Downloading file: atn_74627_bearded-seal_trajectory_20090623-20100318.nc\n",
            "  Downloading file: atn_74630_bearded-seal_trajectory_20090626-20100213.nc\n",
            "  Downloading file: atn_83904_bearded-seal_trajectory_20090625-20120612.nc\n",
            "  Downloading file: atn_99287_bearded-seal_trajectory_20120704-20130312.nc\n",
            "  Downloading file: atn_99310_bearded-seal_trajectory_20110617-20120606.nc\n",
            "\n",
            "--- Recursive Download Finished ---\n"
          ]
        }
      ],
      "source": [
        "# --- Main execution block ---\n",
        "start_url = \"https://www.ncei.noaa.gov/data/oceans/ioos/atn/\"\n",
        "# Create a base directory for all the downloads\n",
        "download_directory = \"data/src/\"\n",
        "\n",
        "print(\"--- Starting Recursive Download ---\")\n",
        "print(f\"Source URL: {start_url}\")\n",
        "print(f\"Local Directory: {download_directory}\\n\")\n",
        "\n",
        "recursive_wget(start_url, download_directory)\n",
        "\n",
        "print(\"\\n--- Recursive Download Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9G3a6N2bMh2"
      },
      "outputs": [],
      "source": [
        "def create_dwc_occurrence(ds, output_csv):\n",
        "\n",
        "  filename = ds.encoding.get('source').split(\"\\\\\")[-1].split(\".\")[0] # \"ioos_atn_{ds.ptt_id}_{start_date}_{end_date}\"\"\n",
        "\n",
        "  dwc_df = pd.DataFrame()\n",
        "  dwc_df['occurrenceID'] = \"ioos_atn_\"+ds.ptt_id+\"_\"+ds['time'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')+\"_\"+ds['z'].astype(str)+\"_\"+ds.animal_common_name.replace(\" \",\"_\")\n",
        "  dwc_df['eventID'] = filename #\"ioos_atn_\"+ds.ptt_id\n",
        "  #dwc_df['eventID'] = ds.ptt_id+\"_\"+ds.animal_common_name.replace(\" \",\"_\") +\"_\"+ds['time'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "  dwc_df['organismID'] = ds.platform_id+\"_\"+ds.animal_common_name.replace(\" \",\"_\")\n",
        "  dwc_df['occurrenceStatus'] = 'present'\n",
        "  dwc_df['basisOfRecord'] = ds['type']\n",
        "  dwc_df['eventDate'] = ds['time'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "  dwc_df['decimalLatitude'] = ds['lat']\n",
        "  dwc_df['decimalLongitude'] = ds['lon']\n",
        "  dwc_df['geodeticDatum'] = ds.crs.epsg_code\n",
        "  dwc_df['scientificName'] = ds['taxon_name'].values.tolist()\n",
        "  dwc_df['scientificNameID'] = ds['taxon_lsid'].values.tolist()\n",
        "  dwc_df['samplingProtocol'] = 'satellite telemetry'\n",
        "  dwc_df['kingdom'] = ds['animal'].attrs['kingdom']\n",
        "  dwc_df['taxonRank'] = ds['animal'].attrs['rank']\n",
        "  dwc_df['lifeStage'] = ds['animal_life_stage'].values.tolist()\n",
        "  dwc_df['sex'] = ds['animal_sex'].values.tolist()\n",
        "  dwc_df['associatedReferences'] = \"https://doi.org/10.25921/wp4e-ph20\"\n",
        "  dwc_df['minimumDepthInMeters'] = ds['z'].values.tolist()\n",
        "  dwc_df['maximumDepthInMeters'] = ds['z'].values.tolist()\n",
        "  dwc_df['bibliographicCitation'] = ds.citation # barking about this.\n",
        "\n",
        "  # set basisOfRecord\n",
        "  dwc_df.loc[dwc_df['basisOfRecord'] == 'User','basisOfRecord'] = 'HumanObservation'\n",
        "  dwc_df.loc[dwc_df['basisOfRecord'] == 'Argos','basisOfRecord'] = 'MachineObservation'\n",
        "  dwc_df.loc[dwc_df['basisOfRecord'] == 'FastGPS','basisOfRecord'] = 'MachineObservation'\n",
        "\n",
        "  # filter to respectable locations\n",
        "  dwc_df['location_class'] = ds['location_class'].to_series()\n",
        "\n",
        "  dwc_df.drop(dwc_df.loc[\n",
        "      (dwc_df['location_class'] == 'A') |\n",
        "      (dwc_df['location_class'] == 'B') |\n",
        "      (dwc_df['location_class'] == 'Z')].index, inplace=True)\n",
        "\n",
        "  # test using xarray\n",
        "  # ds['time'].where((ds['location_class'] != 'A') &\n",
        "  #     (ds['location_class'] != 'B') &\n",
        "  #     (ds['location_class'] != 'Z'),drop=True).values\n",
        "\n",
        "  print(f\"  Extracted {len(dwc_df)} occurrences with valid locations.\")\n",
        "\n",
        "  # assign value to codes\n",
        "  dwc_df.loc[dwc_df['location_class'] == 'nan','location_class'] = 0\n",
        "  dwc_df.loc[dwc_df['location_class'] == 'G','location_class'] = 200\n",
        "  dwc_df.loc[dwc_df['location_class'] == '3','location_class'] = 250\n",
        "  dwc_df.loc[dwc_df['location_class'] == '2','location_class'] = 500\n",
        "  dwc_df.loc[dwc_df['location_class'] == '1','location_class'] = 1500\n",
        "  dwc_df.loc[dwc_df['location_class'] == '0','location_class'] = 10000\n",
        "\n",
        "  # --- Define Occurrences: First detection per location per hour ---\n",
        "  dwc_df['event_hour'] = pd.to_datetime(dwc_df['eventDate']).dt.strftime('%Y-%m-%dT%H')\n",
        "  dwc_df.sort_values('event_hour', inplace=True)\n",
        "  duplicate_counts = dwc_df.groupby(by='event_hour').transform('size')\n",
        "  dwc_df['dataGeneralizations'] = f'first of ' + duplicate_counts.astype(str) + ' records for this hour.'\n",
        "  dwc_df.loc[dwc_df['dataGeneralizations']=='first of 1 records for this hour.','dataGeneralizations'] = ''\n",
        "  dwc_df = dwc_df.drop_duplicates(subset=['event_hour'], keep='first').copy()\n",
        "\n",
        "  dwc_df['occurrenceRemarks'] = 'This is a representative occurrence from a full deployment. For the complete dataset please see https://doi.org/10.25921/wp4e-ph20.'\n",
        "\n",
        "  print(f\"  Extracted {len(dwc_df)} occurrences to first row in hour.\")\n",
        "\n",
        "  # --- Rename a and drop few columns --\n",
        "  dwc_df.rename(columns={'location_class': 'coordinateUncertaintyInMeters',\n",
        "                          },\n",
        "                inplace=True)\n",
        "\n",
        "  dwc_df.drop(columns=['event_hour'], inplace=True)\n",
        "\n",
        "  # only pick specific columns to save\n",
        "  cols = ['eventID', 'occurrenceID', 'occurrenceStatus', 'basisOfRecord',\n",
        "          'organismID', 'eventDate', 'decimalLatitude',\n",
        "          'decimalLongitude', 'geodeticDatum',\n",
        "          'scientificName', 'scientificNameID',\n",
        "          'samplingProtocol', 'kingdom', 'taxonRank', 'lifeStage',\n",
        "          'sex', 'associatedReferences',\n",
        "          'coordinateUncertaintyInMeters', \n",
        "          'minimumDepthInMeters', 'maximumDepthInMeters',\n",
        "          'dataGeneralizations', 'bibliographicCitation',\n",
        "          'occurrenceRemarks']\n",
        "  \n",
        "  # Save the individual CSV\n",
        "  dwc_df.to_csv(output_csv, columns=cols, index=False)\n",
        "  print(f\"  Saved data to '{output_csv}'\")\n",
        "\n",
        "  return dwc_df, cols\n",
        "\n",
        "\n",
        "def create_dwc_event(dwc_df, output_csv):\n",
        "\n",
        "  # create parent event that is a summary of dwc_df\n",
        "  event_df = pd.DataFrame()\n",
        "  event_df['eventID'] = dwc_df['eventID'].unique()\n",
        "  event_df['eventDate'] = dwc_df['eventDate'].min() + '/' + dwc_df['eventDate'].max()\n",
        "  event_df['footprintWKT'] = LineString(list(zip(dwc_df['decimalLongitude'], dwc_df['decimalLatitude'])))\n",
        "  event_df['footprintWKT'] = event_df['footprintWKT'][0].wkt\n",
        "  event_df['footprintWKT'] = event_df['footprintWKT'].str.replace('LINESTRING', 'MULTIPOINT')\n",
        "  event_df['minimumDepthInMeters'] = dwc_df['minimumDepthInMeters'].min()\n",
        "  event_df['maximumDepthInMeters'] = dwc_df['maximumDepthInMeters'].max()\n",
        "  event_df['eventType'] = 'deployment'\n",
        "\n",
        "  if event_df.empty:\n",
        "      print(\"No HumanObservations found in the dataset.\")\n",
        "      return pd.DataFrame()  # Return an empty DataFrame if no observations are found\n",
        "  else:  \n",
        "    print(f\"  found {len(event_df)} HumanObservations.\")\n",
        "    event_df['countryCode'] = 'US'\n",
        "    event_df['samplingProtocol'] = 'satellite telemetry'\n",
        "    \n",
        "    # # initialize Nominatim API - not trusted enough yet\n",
        "    # # see https://nominatim.org/release-docs/develop/api/Reverse/\n",
        "    # from geopy.geocoders import Nominatim\n",
        "    # geolocator = Nominatim(user_agent=\"my_geopy_app\")\n",
        "    # lat = event_df['decimalLatitude'][0].astype(str)\n",
        "    # lon = event_df['decimalLongitude'][0].astype(str)\n",
        "    # location = geolocator.reverse(lat+\",\"+lon)\n",
        "    # event_df['countryCode'] = location.raw['address'].get('country_code').upper()\n",
        "\n",
        "    event_df.to_csv(output_csv.replace(\"occurrence\",\"event\"), index=False)\n",
        "    print(f\"  Created {len(event_df)} events.\")\n",
        "    print(f\"  Saved data to {output_csv.replace('occurrence','event')}\")\n",
        "\n",
        "    return event_df\n",
        "\n",
        "\n",
        "def create_dwc_emof(ds, dwc_df, output_csv):\n",
        "  ## Add more details about the sensor following https://www.gbif.org/occurrence/5068380514\n",
        "  # See https://vocab.nerc.ac.uk/search_nvs/MVB/\n",
        "  # add serial number and all the other details here.\n",
        "  # --- Processing for emof ---\n",
        "  vars = list(ds.keys())\n",
        "  animal_vars = [x for x in vars if re.match(r'animal_(?!life_stage\\b|sex\\b).*',x)]\n",
        "  new_rows = pd.DataFrame()\n",
        "\n",
        "  for animal_var in animal_vars:\n",
        "    row = pd.DataFrame({\n",
        "        'measurementValue': ds[animal_var].values.tolist(),\n",
        "        'measurementType': [f'{animal_var}: {ds[animal_var].long_name}'],\n",
        "        'measurementMethod': ds[animal_var].attrs[animal_var],\n",
        "        'measurementUnit': [ds[animal_var].units if 'units' in ds[animal_var].attrs else ''],\n",
        "    })\n",
        "    new_rows = pd.concat([new_rows,\n",
        "                          row])\n",
        "\n",
        "  emof_df = dwc_df.loc[dwc_df['basisOfRecord']=='HumanObservation',\n",
        "                      ['eventID','occurrenceID']\n",
        "                      ].merge(\n",
        "                          new_rows,\n",
        "                          left_index=True,\n",
        "                          right_index=True)\n",
        "\n",
        "  emof_df.dropna(axis=0, subset=['measurementValue'], inplace=True)\n",
        "\n",
        "  if emof_df.empty:\n",
        "    print(f'  no emof data found')\n",
        "    return pd.DataFrame()  # Return an empty DataFrame if no observations are found\n",
        "  else:\n",
        "    emof_df.to_csv(output_csv.replace(\"occurrence\",\"emof\"), index=False)\n",
        "    print(f\"  Created {len(emof_df)} emofs.\")\n",
        "    print(f\"  Saved data to {output_csv.replace('occurrence','emof')}\")\n",
        "    return emof_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EML generation\n",
        "\n",
        "# borrowed from https://gitlab.oceantrack.org/otn-partner-nodes/ipython-utilities/-/blob/main/dbtools/publish_to_obis.py?ref_type=heads\n",
        "\n",
        "def save_eml_file(eml_metadata:dict) -> str:\n",
        "    \"\"\"\n",
        "    Save EML dictionary in a file\n",
        "    Author: Jon Pye, Angela Dini\n",
        "    Maintainer: Angela Dini\n",
        "    :param eml_metadata: dictionary of EML metadata\n",
        "    :return: filepath of where the EML filepath will be\n",
        "    \"\"\"\n",
        "    # Write it out to the package\n",
        "    template_file = codecs.open('templates/eml.xml.j2', 'r', 'UTF-8').read()\n",
        "    template = Template(template_file)\n",
        "    result_string = template.render(eml_metadata)\n",
        "    eml_file = 'data/dwc/{filename}/eml.xml'.format(**eml_metadata)\n",
        "    fh = codecs.open(eml_file, 'wb+', 'UTF-8')\n",
        "    fh.write(result_string)\n",
        "    fh.close()\n",
        "    eml_full_path = os.path.abspath(eml_file)\n",
        "    print(f\"  EML metadata has been written to '{eml_full_path}'.\")\n",
        "    return eml_full_path\n",
        "\n",
        "def create_eml(ds, df_map):\n",
        "    eml_metadata = ds.attrs\n",
        "\n",
        "    contributors = dict()\n",
        "    for attr in [x for x in ds.attrs if re.match(r'contributor_(?!role_vocabulary\\b).*',x)]:\n",
        "        contributors[attr] = ds.attrs[attr].split(\",\")\n",
        "\n",
        "    contributors_list = [\n",
        "        {key: contributors[key][i] for key in contributors}\n",
        "        for i in range(len(next(iter(contributors.values()))))\n",
        "    ]\n",
        "\n",
        "    other_meta = {\n",
        "        'dataset_ipt_id': None,\n",
        "        'dataset_short_name': ds.encoding.get('source').split(\"\\\\\")[-1].replace(\".nc\",\"\"),\n",
        "        'data_manager_firstname': 'Megan',\n",
        "        'data_manager_lastname': 'McKinzie',\n",
        "        'data_manager_title': 'Data Manager',\n",
        "        'data_manager_phone': '',\n",
        "        'data_manager_email': 'mmckinzie@mbari.org',\n",
        "        'contributors': contributors_list,\n",
        "        'ncei_accession_number': df_map.loc[df_map['file_name'] == ds.encoding.get('source').split(\"\\\\\")[-1], 'accession'].values[0],\n",
        "        'ncei_title': df_map.loc[df_map['file_name'] == ds.encoding.get('source').split(\"\\\\\")[-1], 'title'].values[0],\n",
        "        'filename': ds.encoding.get('source').split(\"\\\\\")[-1].split(\".\")[0],\n",
        "    }\n",
        "\n",
        "    eml_metadata.update(other_meta)\n",
        "\n",
        "    save_eml_file(eml_metadata)\n",
        "    \n",
        "    return eml_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_meta_xml(dwc_df, emof_df, event_df, output_csv, cols):\n",
        "    \"\"\"\n",
        "    Create meta.xml file for the Darwin Core dataset.\n",
        "    \n",
        "    Args:\n",
        "        dwc_df (DataFrame): DataFrame containing Darwin Core occurrence data.\n",
        "        emof_df (DataFrame): DataFrame containing eMoF data.\n",
        "        event_df (DataFrame): DataFrame containing event data.\n",
        "        output_csv (str): Path to the output CSV file.\n",
        "        dir (str): Directory where the meta.xml will be saved.\n",
        "    \"\"\"\n",
        "    # Ensure the directory exists\n",
        "    try:\n",
        "        os.path.exists(output_csv)\n",
        "    except:\n",
        "        print(f\"Missing directory: {output_csv}\")\n",
        "\n",
        "    # create and include the meta.xml and eml.xml\n",
        "    # set the meta.xml paramaters by hand, using the format of the dataframes above\n",
        "    meta_xml_vars = {}\n",
        "\n",
        "    # when writing dwc occurrence file, we only save some columns\n",
        "    dwc_df = dwc_df[cols].copy()\n",
        "    \n",
        "    meta_xml_vars['cols_list'] = dwc_df.columns.tolist()\n",
        "    meta_xml_vars['occurrence_filename'] = output_csv\n",
        "\n",
        "    if not emof_df.empty:\n",
        "        meta_xml_vars ['emof_cols_list'] = emof_df.columns.tolist()\n",
        "        meta_xml_vars['emof_filename'] = output_csv.replace(\"occurrence\",\"emof\")\n",
        "\n",
        "    if not event_df.empty:\n",
        "        meta_xml_vars['event_cols_list'] = event_df.columns.tolist()\n",
        "        meta_xml_vars['event_filename'] = output_csv.replace(\"occurrence\",\"event\")\n",
        "\n",
        "    # grab the template file for making meta.xml\n",
        "    meta_template_file = codecs.open('templates/meta.xml.j2', 'r', 'UTF-8').read()\n",
        "    meta_template = Template(meta_template_file)\n",
        "    meta_result_string = meta_template.render(meta_xml_vars)\n",
        "    dir = os.path.join(*output_csv.split(\"\\\\\")[:-1])\n",
        "    meta_file = f'{dir}/meta.xml'\n",
        "\n",
        "    fh = codecs.open(meta_file, 'wb+', 'UTF-8')\n",
        "    fh.write(meta_result_string)\n",
        "    fh.close()\n",
        "    meta_full_path = os.path.abspath(meta_file)\n",
        "    print(f\"  Meta XML has been written to '{meta_full_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def package_dwc_zip(output_dir=\"data/dwc\", zip_filename=\"data/dwc_package.zip\"):\n",
        "    \"\"\"\n",
        "    Packages all CSV and XML files in the specified output directory into a zip file.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): The directory containing the files to package.\n",
        "        zip_filename (str): The path for the output zip file.\n",
        "    \"\"\"\n",
        "    print(f\"  Packaging Darwin Core files from '{output_dir}' into '{zip_filename}'...\")\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(output_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(('.csv', '.xml')):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, output_dir)\n",
        "                    zipf.write(file_path, arcname)\n",
        "    print(f\"  ✅ Packaged files into '{zip_filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aD62GgzSS0zu"
      },
      "outputs": [],
      "source": [
        "def convert_to_dwc_individual(file_paths, output_dir=\"data/dwc\"):\n",
        "    \"\"\"\n",
        "    Converts a list of NetCDF files to individual Darwin Core Occurrence CSVs.\n",
        "\n",
        "    An \"occurrence\" is the first detection of an animal at a specific\n",
        "    location within a given hour.\n",
        "\n",
        "    Args:\n",
        "        file_paths (list): A list of paths to the .nc files.\n",
        "        output_dir (str): The directory to save the individual CSV files.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- 2. Starting Darwin Core Conversion (Individual Files) ---\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "    processed_count = 0\n",
        "\n",
        "    for nc_file in file_paths:\n",
        "\n",
        "        base_filename = os.path.basename(nc_file)\n",
        "        sub_dir = base_filename.split('.')[0]\n",
        "\n",
        "        if not os.path.exists(f\"{output_dir}/{sub_dir}\"):\n",
        "          os.makedirs(f\"{output_dir}/{sub_dir}\")\n",
        "          print(f\"Created output directory: {output_dir}/{sub_dir}\")\n",
        "\n",
        "        output_csv = os.path.join(output_dir, f\"{sub_dir}/{os.path.splitext(base_filename)[0]}_occurrence.csv\")\n",
        "        output_csv = os.path.normpath(output_csv)\n",
        "        #output_dir = os.path.join(*output_csv.split(\"\\\\\")[:-1])\n",
        "        print(f\"Processing {base_filename}...\")\n",
        "\n",
        "        try:\n",
        "            with xr.open_dataset(nc_file, engine='netcdf4') as ds:\n",
        "                df = ds.to_dataframe().reset_index()\n",
        "\n",
        "                print(f\"Found {len(df)} records.\")\n",
        "\n",
        "                # --- Data Cleaning and Preparation ---\n",
        "                if 'lat' not in df.columns or 'lon' not in df.columns:\n",
        "                    print(f\"  Skipping {base_filename}: missing location data.\")\n",
        "                    continue\n",
        "\n",
        "                df.dropna(subset=['lat', 'lon', 'time'], inplace=True)\n",
        "                if df.empty:\n",
        "                    print(f\"  Skipping {base_filename}: no valid records.\")\n",
        "                    continue\n",
        "\n",
        "                # --- Map to Darwin Core Occurrence Terms ---\n",
        "                dwc_df, cols = create_dwc_occurrence(ds, output_csv)\n",
        "\n",
        "                # Create and save eml\n",
        "                create_eml(ds, df_map)\n",
        "\n",
        "                # --- Event and eMoF (as needed) ---\n",
        "                event_df = create_dwc_event(dwc_df, output_csv)\n",
        "                emof_df = create_dwc_emof(ds, dwc_df, output_csv)\n",
        "\n",
        "                # --- Create meta.xml file ---\n",
        "                create_meta_xml(dwc_df, emof_df, event_df, output_csv, cols)\n",
        "\n",
        "                # --- Package into DwC-A ---\n",
        "                output_dir_zip = f\"data/dwc/{sub_dir}/\"\n",
        "                zip_filename=f\"data/dwc/{sub_dir}/{base_filename.replace('.nc','.zip')}\"\n",
        "                package_dwc_zip(output_dir=output_dir_zip, zip_filename=zip_filename)\n",
        "\n",
        "                processed_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not process {base_filename}: {e}\")\n",
        "\n",
        "    print(f\"\\n--- 3. Conversion Complete ---\")\n",
        "    print(f\"✅ Success! Processed {processed_count} files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHCDd3D4dSFv"
      },
      "source": [
        "Convert data to DarwinCore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VByr8WDedRja",
        "outputId": "d210d9a1-2f5e-4059-89e7-c5fa080226f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 2. Starting Darwin Core Conversion (Individual Files) ---\n",
            "Processing atn_137491_spotted-seal_trajectory_20180418-20180526.nc...\n",
            "Found 107 records.\n",
            "  Extracted 12 occurrences with valid locations.\n",
            "  Extracted 5 occurrences to first row in hour.\n",
            "  Saved data to 'data\\dwc\\atn_137491_spotted-seal_trajectory_20180418-20180526\\atn_137491_spotted-seal_trajectory_20180418-20180526_occurrence.csv'\n",
            "  EML metadata has been written to 'c:\\Users\\Mathew.Biddle\\Documents\\GitProjects\\bio_data_guide\\datasets\\atn_satellite_telemetry\\data\\dwc\\atn_137491_spotted-seal_trajectory_20180418-20180526\\eml.xml'.\n",
            "  found 1 HumanObservations.\n",
            "  Created 1 events.\n",
            "  Saved data to data\\dwc\\atn_137491_spotted-seal_trajectory_20180418-20180526\\atn_137491_spotted-seal_trajectory_20180418-20180526_event.csv\n",
            "  Created 2 emofs.\n",
            "  Saved data to data\\dwc\\atn_137491_spotted-seal_trajectory_20180418-20180526\\atn_137491_spotted-seal_trajectory_20180418-20180526_emof.csv\n",
            "  Meta XML has been written to 'c:\\Users\\Mathew.Biddle\\Documents\\GitProjects\\bio_data_guide\\datasets\\atn_satellite_telemetry\\data\\dwc\\atn_137491_spotted-seal_trajectory_20180418-20180526\\meta.xml'.\n",
            "  Packaging Darwin Core files from 'data/dwc/atn_137491_spotted-seal_trajectory_20180418-20180526/' into 'data/dwc/atn_137491_spotted-seal_trajectory_20180418-20180526/atn_137491_spotted-seal_trajectory_20180418-20180526.zip'...\n",
            "  ✅ Packaged files into 'data/dwc/atn_137491_spotted-seal_trajectory_20180418-20180526/atn_137491_spotted-seal_trajectory_20180418-20180526.zip'\n",
            "Processing atn_137494_ribbon-seal_trajectory_20140426-20140426.nc...\n",
            "Found 23 records.\n",
            "  Extracted 11 occurrences with valid locations.\n",
            "  Extracted 4 occurrences to first row in hour.\n",
            "  Saved data to 'data\\dwc\\atn_137494_ribbon-seal_trajectory_20140426-20140426\\atn_137494_ribbon-seal_trajectory_20140426-20140426_occurrence.csv'\n",
            "  EML metadata has been written to 'c:\\Users\\Mathew.Biddle\\Documents\\GitProjects\\bio_data_guide\\datasets\\atn_satellite_telemetry\\data\\dwc\\atn_137494_ribbon-seal_trajectory_20140426-20140426\\eml.xml'.\n",
            "  found 1 HumanObservations.\n",
            "  Created 1 events.\n",
            "  Saved data to data\\dwc\\atn_137494_ribbon-seal_trajectory_20140426-20140426\\atn_137494_ribbon-seal_trajectory_20140426-20140426_event.csv\n",
            "  no emof data found\n",
            "  Meta XML has been written to 'c:\\Users\\Mathew.Biddle\\Documents\\GitProjects\\bio_data_guide\\datasets\\atn_satellite_telemetry\\data\\dwc\\atn_137494_ribbon-seal_trajectory_20140426-20140426\\meta.xml'.\n",
            "  Packaging Darwin Core files from 'data/dwc/atn_137494_ribbon-seal_trajectory_20140426-20140426/' into 'data/dwc/atn_137494_ribbon-seal_trajectory_20140426-20140426/atn_137494_ribbon-seal_trajectory_20140426-20140426.zip'...\n",
            "  ✅ Packaged files into 'data/dwc/atn_137494_ribbon-seal_trajectory_20140426-20140426/atn_137494_ribbon-seal_trajectory_20140426-20140426.zip'\n",
            "Processing atn_38553_bearded-seal_trajectory_20110618-20120314.nc...\n",
            "Found 10197 records.\n",
            "  Extracted 1871 occurrences with valid locations.\n",
            "  Extracted 1218 occurrences to first row in hour.\n",
            "  Saved data to 'data\\dwc\\atn_38553_bearded-seal_trajectory_20110618-20120314\\atn_38553_bearded-seal_trajectory_20110618-20120314_occurrence.csv'\n",
            "  EML metadata has been written to 'c:\\Users\\Mathew.Biddle\\Documents\\GitProjects\\bio_data_guide\\datasets\\atn_satellite_telemetry\\data\\dwc\\atn_38553_bearded-seal_trajectory_20110618-20120314\\eml.xml'.\n",
            "  found 1 HumanObservations.\n",
            "  Created 1 events.\n",
            "  Saved data to data\\dwc\\atn_38553_bearded-seal_trajectory_20110618-20120314\\atn_38553_bearded-seal_trajectory_20110618-20120314_event.csv\n",
            "  no emof data found\n",
            "  Meta XML has been written to 'c:\\Users\\Mathew.Biddle\\Documents\\GitProjects\\bio_data_guide\\datasets\\atn_satellite_telemetry\\data\\dwc\\atn_38553_bearded-seal_trajectory_20110618-20120314\\meta.xml'.\n",
            "  Packaging Darwin Core files from 'data/dwc/atn_38553_bearded-seal_trajectory_20110618-20120314/' into 'data/dwc/atn_38553_bearded-seal_trajectory_20110618-20120314/atn_38553_bearded-seal_trajectory_20110618-20120314.zip'...\n",
            "  ✅ Packaged files into 'data/dwc/atn_38553_bearded-seal_trajectory_20110618-20120314/atn_38553_bearded-seal_trajectory_20110618-20120314.zip'\n",
            "\n",
            "--- 3. Conversion Complete ---\n",
            "✅ Success! Processed 3 files.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download all .nc files from the URL\n",
        "#local_files = glob.glob('data\\\\src\\\\*.nc')#[:10]\n",
        "\n",
        "local_files = ['data\\\\src\\\\atn_137491_spotted-seal_trajectory_20180418-20180526.nc',\n",
        "               'data\\\\src\\\\atn_137494_ribbon-seal_trajectory_20140426-20140426.nc',\n",
        "               'data\\\\src\\\\atn_38553_bearded-seal_trajectory_20110618-20120314.nc'\n",
        "             ]\n",
        "# Step 2: Convert the downloaded files to individual Darwin Core CSVs\n",
        "if local_files:\n",
        "    convert_to_dwc_individual(local_files)\n",
        "else:\n",
        "    print(\"No files were downloaded, so conversion cannot proceed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push to IPT following\n",
        "\n",
        "https://github.com/cioos-siooc/pyobistools/blob/ipt_publishing/pyobistools/publish_to_ipt.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: extend this to allow checking for different versions of the IPT as the forms may change?\n",
        "# Functional for IPT 2.6.3\n",
        "\n",
        "def open_ipt_session(ipt_auth, ipt_url):\n",
        "    \"\"\"\n",
        "    Begin a session with the target IPT\n",
        "    Author: Jon Pye\n",
        "    :param ipt_auth: Authentication details for the ipt, of the form {'email': 'email@mailserver.com', 'password':'cleartextPassword'}\n",
        "    :param ipt_url: URL of the IPT we are authenticating with.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    \n",
        "    # relative path to IPT login form\n",
        "    login_url = ipt_url + 'login.do'\n",
        "    \n",
        "    s = requests.Session()  # open a session\n",
        "    \n",
        "    # retrieve the login form\n",
        "    resp = s.get(login_url)\n",
        "    \n",
        "    # login forms generate a CSRF token that we have to persist in our response  \n",
        "    soup = BeautifulSoup(resp.text, 'lxml')\n",
        "    \n",
        "    # Add it to our credentials dictionary\n",
        "    ipt_auth['csrfToken'] = soup.find(\"input\", {\"name\": \"csrfToken\"})['value']\n",
        "    login = s.post(login_url, data=ipt_auth)\n",
        "    \n",
        "    if login.status_code != 200:\n",
        "        print(\"Login failed, status code {}\".format(login.status_code))\n",
        "        print(login.text)\n",
        "        return None\n",
        "    else:\n",
        "        return s\n",
        "\n",
        "\n",
        "def create_new_ipt_project(projname: str, filepath: str, ipt_url: str, ipt_session):\n",
        "    \n",
        "    \"\"\"\n",
        "    Create a new project on the given IPT using an existing DwC archive zip\n",
        "    Author: Jon Pye\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param filepath: payload resource filepath\n",
        "    :param ipt_url: URL of the IPT to publish to\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "    :return: URL of the resource\n",
        "    \"\"\"\n",
        "    \n",
        "    path, filename = os.path.split(filepath)\n",
        "    \n",
        "    if not filename:  # if the filepath has no name in it\n",
        "        print('no file specified in filepath, aborting')\n",
        "        return None\n",
        "    else:\n",
        "        print(path, filename)\n",
        "        print(filepath)\n",
        "    \n",
        "    # if there IS a file and it is not a valid DwC Archive, do we want to do anything here? The IPT runs its own checks...\n",
        "    \n",
        "    values = MultipartEncoder(fields={'create': 'Create',  # hidden form fields with values\n",
        "                                      'shortname': projname,\n",
        "                                      'resourceType': 'samplingevent',\n",
        "                                      '__checkbox_importDwca': 'true',\n",
        "                                      'importDwca': 'true',\n",
        "                                      'file': (filename, \n",
        "                                               open(filepath, 'rb'),\n",
        "                                               'application/x-zip-compressed'),\n",
        "                                     }\n",
        "                             )\n",
        "    create_dataset = ipt_session.post(ipt_url + 'manage/create.do',\n",
        "                                      data=values,\n",
        "                                      headers={'Content-Type': values.content_type}\n",
        "                                     )\n",
        "    return create_dataset\n",
        "\n",
        "def refresh_ipt_project_files(projname: str, filepath: str, ipt_url: str, ipt_session):\n",
        "    \"\"\"\n",
        "    TODO: This should only push the .csv files, not the eml and meta\n",
        "    \n",
        "    Update data for a project on the given IPT using an existing DwC archive zip\n",
        "    Author: Jon Pye\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param filepath: payload resource filepath\n",
        "    :param ipt_url: URL of the IPT to publish to\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "    :return: URL of the resource\n",
        "    \"\"\"\n",
        "    \n",
        "    path, filename = os.path.split(filepath)\n",
        "    \n",
        "    if not filename:  # if the filepath has no name in it\n",
        "        print('no file specified in filepath, aborting')\n",
        "        return None\n",
        "    \n",
        "    values = MultipartEncoder(fields={  'add': 'Add',\n",
        "                                        'r': projname,\n",
        "                                        'sourceType': 'source-file',\n",
        "                                        'validate': 'false', \n",
        "                                        'file': (filename,\n",
        "                                                 open(filepath, 'rb'),\n",
        "                                                 'application/x-zip-compressed'),\n",
        "                                     })\n",
        "    \n",
        "    update_dataset = ipt_session.post(ipt_url + 'manage/addsource.do',\n",
        "                                      data=values, \n",
        "                                      headers = {'Content-Type': values.content_type}\n",
        "                                     )\n",
        "    if update_dataset.status_code == 200:\n",
        "        # Handle the Are you Sure popup.        \n",
        "        print(\"Publication successful\")\n",
        "        return update_dataset\n",
        "    else:\n",
        "        print(\"publication error, check landing page output\")\n",
        "        return update_dataset\n",
        "\n",
        "\n",
        "def refresh_ipt_project_metadata(projname: str, filepath: str, ipt_url: str, ipt_session):\n",
        "    \"\"\"\n",
        "    Update metadata for a project on the given IPT using an existing eml.xml file\n",
        "    Author: Jon Pye\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param filepath: payload resource filepath\n",
        "    :param ipt_url: URL of the IPT to publish to\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "    :return: URL of the resource\n",
        "    \"\"\"\n",
        "    \n",
        "    path, filename = os.path.split(filepath)\n",
        "    \n",
        "    if not filename:  # if the filepath has no name in it\n",
        "        print('no file specified in filepath, aborting')\n",
        "        return None\n",
        "    \n",
        "    values = MultipartEncoder(fields={  'emlReplace': 'Replace',\n",
        "                                        'r': projname,\n",
        "                                        'sourceType': 'source-file',\n",
        "                                        'validateEml': 'true',\n",
        "                                        '__checkbox_validateEml': 'true',\n",
        "                                        'emlFile': (filename,\n",
        "                                                    open(filepath, 'rb'),\n",
        "                                                    'application/xml'),\n",
        "                                     })\n",
        "\n",
        "    update_metadata = ipt_session.post(ipt_url + 'manage/replace-eml.do',\n",
        "                                       data=values, \n",
        "                                       headers = {'Content-Type':values.content_type}\n",
        "                                      )\n",
        "    return update_metadata\n",
        "\n",
        "def change_publishing_org_ipt_project(projname: str, ipt_url: str, ipt_session, new_publishing_org_name: str):\n",
        "    \"\"\"\n",
        "    Change the publishing organization in the given IPT project\n",
        "    Author: Mathew Biddle\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param ipt_url: URL of the IPT to publish to\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "    :param new_publishing_org: the new publishing organisation to set for this project. See publishingOrganizationKey in the IPT source.\n",
        "\n",
        "    :return: URL of the resource\n",
        "    \"\"\"\n",
        "    pub_orgs = {'NOAA Integrated Ocean Observing System': \"1d38bb22-cbea-4845-8b0c-f62551076080\",\n",
        "                 'No organization': \"625a5522-1886-4998-be46-52c66dd566c9\",\n",
        "                 'SCAR - AntOBIS': \"104e9c96-791b-4f14-978c-f581cb214912\",\n",
        "                 'The Marine Genome Project': \"aa0b26e8-779c-4645-a569-5f39fa85d528\",\n",
        "                 'USFWS-AK': \"530fda11-7af7-4447-9649-0f9fc22e6156\",\n",
        "                 'United States Fish and Wildlife Service': \"f8dbeca7-3131-41ab-872f-bfad71041f3f\",\n",
        "                 'United States Geological Survey': \"c3ad790a-d426-4ac1-8e32-da61f81f0117\",\n",
        "                }\n",
        "\n",
        "    if new_publishing_org_name not in pub_orgs:\n",
        "        print(f\"Publishing organization '{new_publishing_org_name}' not recognised as one of {pub_orgs.keys()}. Please check the name and try again.\")\n",
        "        return None\n",
        "\n",
        "    pub_params = {'r' : projname,          # resource = dataset name\n",
        "                  'publishingOrganizationKey': pub_orgs[new_publishing_org_name],\n",
        "                 }\n",
        "    \n",
        "    contents = ipt_session.post(ipt_url + 'manage/resource-changePublishingOrganization.do', data = pub_params)\n",
        "    return contents\n",
        "\n",
        "\n",
        "def make_public_ipt_project(projname: str, ipt_url: str, ipt_session):\n",
        "    \"\"\"\n",
        "    Update metadata for a project on the given IPT\n",
        "    Author: Jon Pye\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param ipt_url: URL of the IPT to publish to\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "    :return: URL of the resource\n",
        "    \"\"\"\n",
        "    pub_params = {'r' : projname,          # resource = dataset name\n",
        "                  'makePrivate': 'Public'\n",
        "                 }\n",
        "    \n",
        "    contents = ipt_session.post(ipt_url + 'manage/resource-makePublic.do', data = pub_params)\n",
        "    return contents\n",
        "\n",
        "\n",
        "def publish_ipt_project(projname: str, ipt_url: str, ipt_session, publishing_notes: str = \"\"):\n",
        "    \"\"\"\n",
        "    Update metadata for a project on the given IPT\n",
        "    Author: Jon Pye\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param ipt_url: URL of the IPT to publish to\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "    :param publishing_notes: optional message to publish this version with\n",
        "    :return: URL of the resource\n",
        "    \"\"\"\n",
        "    \n",
        "    pub_params = {'r' : projname,      # resource = dataset name\n",
        "                  'autopublish': '',\n",
        "                  'currPubMode' : 'AUTO_PUBLISH_OFF',\n",
        "                  'pubMode': '',\n",
        "                  'currPubFreq': '',\n",
        "                  'pubFreq': '',\n",
        "                  'publish': 'Publish',\n",
        "                  'summary': publishing_notes\n",
        "             }\n",
        "    contents = ipt_session.post(ipt_url + 'manage/publish.do', data = pub_params)\n",
        "    return contents\n",
        "\n",
        "def register_ipt_project(projname: str, ipt_url: str, ipt_session):\n",
        "    \"\"\"\n",
        "    Update Register the given IPT project with GBIF\n",
        "    Author: Mathew Biddle\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param ipt_url: URL of the IPT to publish to\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "\n",
        "    Need to do the following in the dialog-confirm. \n",
        "       * check checkbox-confirm\n",
        "       * select yes-button\n",
        "\n",
        "    :return: URL of the resource\n",
        "    \"\"\"\n",
        "\n",
        "    pub_params = {'r' : projname,          # resource = dataset name\n",
        "                  'checkbox-confirm': 'true',  # checkbox-confirm\n",
        "                  'yes-button': 'Yes',\n",
        "                 }\n",
        "    \n",
        "    contents = ipt_session.post(ipt_url + 'manage/resource-registerResource.do', data = pub_params)\n",
        "    return contents\n",
        "\n",
        "def check_if_project_exists(projname: str, ipt_url: str, ipt_session):\n",
        "    \"\"\"\n",
        "    Test if a project exists on the IPT already\n",
        "    Author: Jon Pye\n",
        "    :param projname: the project name as given by get_obis_shortname()\n",
        "    :param ipt_url: URL of the IPT to check for this publication\n",
        "    :param ipt_session: authenticated requests session for the IPT\n",
        "    :return: True if the project already exists on the IPT in question\n",
        "    \"\"\"\n",
        "\n",
        "    checkUrl = '{ipt_url}ipt/resource?r={projname}'.format(ipt_url=ipt_url, projname=projname)\n",
        "\n",
        "    contents = ipt_session.post(checkUrl)\n",
        "\n",
        "    # if it's not found, the IPT returns a 404\n",
        "    if contents.status_code == 404:\n",
        "        print(\"No existing repository by this name: '{}'\".format(projname))\n",
        "        return False\n",
        "    elif contents.status_code == 200:\n",
        "        print(\"Found existing project by name: '{}'\".format(projname))\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing project by name: 'atn_38553_bearded-seal_trajectory_20110618-20120314'\n",
            "Changing publishing organization to: NOAA Integrated Ocean Observing System\n",
            "Found existing project by name: 'atn_137491_spotted-seal_trajectory_20180418-20180526'\n",
            "Changing publishing organization to: NOAA Integrated Ocean Observing System\n",
            "Found existing project by name: 'atn_137494_ribbon-seal_trajectory_20140426-20140426'\n",
            "Changing publishing organization to: NOAA Integrated Ocean Observing System\n"
          ]
        }
      ],
      "source": [
        "config = dotenv_values(\".env\")\n",
        "\n",
        "ipt_auth = {\n",
        "    'email': config['IPT_ADMIN_EMAIL'],\n",
        "    'password': config['IPT_PASSWORD'],\n",
        "}\n",
        "\n",
        "ipt_url = 'https://ipt-obis.gbif.us/'\n",
        "\n",
        "ipt_session = open_ipt_session(ipt_auth, ipt_url)\n",
        "\n",
        "packages = ['data/dwc/atn_38553_bearded-seal_trajectory_20110618-20120314/atn_38553_bearded-seal_trajectory_20110618-20120314.zip',\n",
        "            'data/dwc/atn_137491_spotted-seal_trajectory_20180418-20180526/atn_137491_spotted-seal_trajectory_20180418-20180526.zip',\n",
        "            'data/dwc/atn_137494_ribbon-seal_trajectory_20140426-20140426/atn_137494_ribbon-seal_trajectory_20140426-20140426.zip']\n",
        "\n",
        "for filepath in packages:\n",
        "\n",
        "    projname = filepath.split(\"/\")[-1].replace(\".zip\",\"\")\n",
        "\n",
        "    # Create/refresh IPT project\n",
        "    if check_if_project_exists(projname, ipt_url, ipt_session):\n",
        "        ans = input(f\"Project {projname} already exists. Do you want to refresh it? (y/n): \")\n",
        "        if ans.lower() in ['y', 'yes']:\n",
        "            refresh_ipt_project_files(projname, filepath, ipt_url, ipt_session)\n",
        "            eml_file = \"/\".join(filepath.split(\"/\")[:-1])+\"/eml.xml\"\n",
        "            refresh_ipt_project_metadata(projname, eml_file, ipt_url, ipt_session)\n",
        "    else:\n",
        "        ans = input(f\"Project {projname} already exists. Do you want to create it? (y/n): \")\n",
        "        if ans.lower() in ['y', 'yes']:\n",
        "            print(f\"Creating new IPT project: {projname}\")\n",
        "            create_new_ipt_project(projname, filepath, ipt_url, ipt_session)\n",
        "            \n",
        "            \n",
        "    new_publishing_org_name = \"NOAA Integrated Ocean Observing System\"\n",
        "    ans = input(f\"Do you want to change the publishing org to {new_publishing_org_name}? (y/n): \")\n",
        "    if ans.lower() in ['y', 'yes']:\n",
        "        print(f\"Changing publishing organization to: {new_publishing_org_name}\")\n",
        "        change_publishing_org_ipt_project(projname, ipt_url, ipt_session, new_publishing_org_name)\n",
        "\n",
        "\n",
        "    ans = input(f\"Do you want to make the IPT project files public? (y/n): \")\n",
        "    if ans.lower() in ['y', 'yes']:\n",
        "        print(f\"Making public IPT project: {projname}\")\n",
        "        make_public_ipt_project(projname, ipt_url, ipt_session)\n",
        "\n",
        "    ans = input(f\"Do you want to publish the IPT project files? (y/n): \")\n",
        "    if ans.lower() in ['y', 'yes']:\n",
        "        publishing_notes = \"Published using the IOOS ATN IPT publishing script.\"\n",
        "        print(f\"Publishing IPT project: {projname}\")\n",
        "        publish_ipt_project(projname, ipt_url, ipt_session, publishing_notes)\n",
        "\n",
        "    ans = input(f\"Do you want to register the IPT project with GBIF? (y/n): \")\n",
        "    if ans.lower() in ['y', 'yes']:\n",
        "        print(f\"Registering IPT project: {projname}\")\n",
        "        register_ipt_project(projname, ipt_url, ipt_session)\n",
        "\n",
        "        # TODO Check with Steve if we need to republish after registering\n",
        "        # publishing_notes = \"Registering with GBIF\"\n",
        "\n",
        "        # print(f\"Publishing IPT project: {projname} again...\")\n",
        "        # publish_ipt_project(projname, ipt_url, ipt_session, publishing_notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IOOS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
